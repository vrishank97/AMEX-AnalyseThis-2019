{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "leaderboard = pd.read_csv(\"leaderboard_dataset.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "train = train.fillna(0)\n",
    "leaderboard = leaderboard.fillna(0)\n",
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train[\"VAR21\"]\n",
    "X = train.drop([\"VAR21\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for c in X.columns:\n",
    "    if X[c].dtype == 'object':\n",
    "        lbl = LabelEncoder() \n",
    "        lbl.fit(list(X[c].values) + list(test[c].values) + list(leaderboard[c].values)) \n",
    "        X[c] = lbl.transform(list(X[c].values))\n",
    "        test[c] = lbl.transform(list(test[c].values))\n",
    "        leaderboard[c] = lbl.transform(list(leaderboard[c].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lbl = LabelEncoder() \n",
    "lbl.fit(list(y.values)) \n",
    "y = lbl.transform(list(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "X[\"Target\"] = y\n",
    "# High\n",
    "X[\"Target\"][X[\"Target\"] == 0] = 3\n",
    "# Low\n",
    "X[\"Target\"][X[\"Target\"] == 1] = 0\n",
    "# Medium\n",
    "X[\"Target\"][X[\"Target\"] == 2] = 1\n",
    "# High\n",
    "X[\"Target\"][X[\"Target\"] == 3] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "X[\"Target\"] = y\n",
    "X1 = deepcopy(X)\n",
    "X2 = deepcopy(X)\n",
    "# High\n",
    "X1[\"Target\"][X1[\"Target\"] == 0] = 3\n",
    "# Low\n",
    "X1[\"Target\"][X1[\"Target\"] == 1] = 0\n",
    "# Medium\n",
    "X1[\"Target\"][X1[\"Target\"] == 2] = 1\n",
    "# High\n",
    "X1[\"Target\"][X1[\"Target\"] == 3] = 2\n",
    "\n",
    "X1 = X1[X1[\"Target\"]!=0]\n",
    "\n",
    "X1[\"Target\"][X1[\"Target\"] == 1] = 0\n",
    "X1[\"Target\"][X1[\"Target\"] == 2] = 1\n",
    "\n",
    "\n",
    "# High\n",
    "X2[\"Target\"][X2[\"Target\"] == 0] = 3\n",
    "# Low\n",
    "X2[\"Target\"][X2[\"Target\"] == 1] = 0\n",
    "# Medium\n",
    "X2[\"Target\"][X2[\"Target\"] == 2] = 1\n",
    "# High\n",
    "X2[\"Target\"][X2[\"Target\"] == 3] = 2\n",
    "\n",
    "X2 = X2[X2[\"Target\"]!=2]\n",
    "\n",
    "X2[\"Target\"][X2[\"Target\"] == 0] = 0\n",
    "X2[\"Target\"][X2[\"Target\"] == 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    30253\n",
      "1     3747\n",
      "Name: Target, dtype: int64\n",
      "1    21919\n",
      "0    12081\n",
      "Name: Target, dtype: int64\n",
      "(34000,)\n",
      "(34000,)\n",
      "[0]\ttrain-error:0.105824+0.00135523\ttest-error:0.111529+0.00278202\n",
      "[50]\ttrain-error:0.104353+0.00160044\ttest-error:0.109441+0.00307009\n",
      "[100]\ttrain-error:0.104132+0.00191566\ttest-error:0.109412+0.00318923\n",
      "[150]\ttrain-error:0.104162+0.00193535\ttest-error:0.109383+0.00322068\n",
      "[200]\ttrain-error:0.104309+0.00183152\ttest-error:0.109177+0.00362709\n",
      "[250]\ttrain-error:0.104074+0.00187858\ttest-error:0.109088+0.00372337\n",
      "[300]\ttrain-error:0.103647+0.00182814\ttest-error:0.109059+0.00384747\n",
      "[350]\ttrain-error:0.103132+0.00177806\ttest-error:0.109177+0.00384023\n",
      "[400]\ttrain-error:0.102706+0.0018443\ttest-error:0.109118+0.00385507\n",
      "[450]\ttrain-error:0.102147+0.00172904\ttest-error:0.109147+0.0036826\n",
      "[500]\ttrain-error:0.101794+0.00172788\ttest-error:0.109+0.00386309\n",
      "[550]\ttrain-error:0.101294+0.00173825\ttest-error:0.109029+0.0039732\n",
      "[600]\ttrain-error:0.100809+0.00178485\ttest-error:0.108941+0.00395101\n",
      "[650]\ttrain-error:0.100206+0.00158185\ttest-error:0.108794+0.00396527\n",
      "[700]\ttrain-error:0.099706+0.00151105\ttest-error:0.108912+0.00384023\n",
      "[750]\ttrain-error:0.099235+0.00138551\ttest-error:0.108912+0.00391153\n",
      "[800]\ttrain-error:0.0987793+0.00128696\ttest-error:0.109+0.00386309\n",
      "[850]\ttrain-error:0.0984117+0.00126648\ttest-error:0.108883+0.00389463\n",
      "[900]\ttrain-error:0.0980147+0.00128227\ttest-error:0.108794+0.00396527\n",
      "[950]\ttrain-error:0.097853+0.00133598\ttest-error:0.108853+0.00392601\n",
      "[1000]\ttrain-error:0.097353+0.00135441\ttest-error:0.108971+0.00403577\n",
      "[1050]\ttrain-error:0.0970293+0.00129879\ttest-error:0.108912+0.00400439\n",
      "[1100]\ttrain-error:0.0966767+0.00113731\ttest-error:0.108883+0.00389463\n",
      "[1150]\ttrain-error:0.0963237+0.00106553\ttest-error:0.108824+0.00393398\n",
      "[1199]\ttrain-error:0.096044+0.00101201\ttest-error:0.108765+0.00388043\n",
      "num_boost_rounds=1200\n",
      "[0]\ttrain-error:0.334397+0.00327324\ttest-error:0.3425+0.0039003\n",
      "[50]\ttrain-error:0.323353+0.00220506\ttest-error:0.332941+0.00250111\n",
      "[100]\ttrain-error:0.319956+0.00204185\ttest-error:0.330206+0.00336479\n",
      "[150]\ttrain-error:0.316794+0.00158933\ttest-error:0.3285+0.00216514\n",
      "[200]\ttrain-error:0.313971+0.00212548\ttest-error:0.327088+0.00201499\n",
      "[250]\ttrain-error:0.311147+0.00113967\ttest-error:0.326412+0.00229429\n",
      "[300]\ttrain-error:0.308647+0.00187526\ttest-error:0.325588+0.00273075\n",
      "[350]\ttrain-error:0.30547+0.00151759\ttest-error:0.325029+0.00156056\n",
      "[400]\ttrain-error:0.302294+0.00147583\ttest-error:0.324176+0.00175956\n",
      "[450]\ttrain-error:0.300147+0.000606764\ttest-error:0.323559+0.00157172\n",
      "[500]\ttrain-error:0.298044+0.000399286\ttest-error:0.323382+0.000626664\n",
      "[550]\ttrain-error:0.295206+0.000296892\ttest-error:0.323353+0.0010208\n",
      "[600]\ttrain-error:0.292544+0.000360673\ttest-error:0.323382+0.000915045\n",
      "[650]\ttrain-error:0.290382+0.00026575\ttest-error:0.323353+0.00137724\n",
      "[700]\ttrain-error:0.288559+0.000319263\ttest-error:0.323412+0.001033\n",
      "[750]\ttrain-error:0.286794+5.97048e-05\ttest-error:0.323235+0.00130681\n",
      "[800]\ttrain-error:0.284765+0.000212403\ttest-error:0.322589+0.00168807\n",
      "[850]\ttrain-error:0.282662+0.000402791\ttest-error:0.322971+0.00148492\n",
      "[900]\ttrain-error:0.280647+0.000914554\ttest-error:0.323177+0.00144372\n",
      "[950]\ttrain-error:0.278765+0.00132548\ttest-error:0.323029+0.00176022\n",
      "[1000]\ttrain-error:0.276824+0.00160435\ttest-error:0.323147+0.00159544\n",
      "[1050]\ttrain-error:0.274897+0.0011514\ttest-error:0.322971+0.00168793\n",
      "[1100]\ttrain-error:0.272794+0.00117386\ttest-error:0.323059+0.00153462\n",
      "[1150]\ttrain-error:0.27072+0.00136856\ttest-error:0.323294+0.00168851\n",
      "[1199]\ttrain-error:0.268882+0.00116226\ttest-error:0.323294+0.00152482\n",
      "num_boost_rounds=1200\n"
     ]
    }
   ],
   "source": [
    "print(X1[\"Target\"].value_counts())\n",
    "print(X2[\"Target\"].value_counts())\n",
    "print(X1[\"Target\"].shape)\n",
    "print(X2[\"Target\"].shape)\n",
    "\n",
    "xgb_params = {\n",
    "    'n_trees': 700, \n",
    "    'eta': 0.01,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.93,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'error',\n",
    "    'silent': 1,\n",
    "}\n",
    "\n",
    "dtrain1 = xgb.DMatrix(X1.drop([\"Target\"], axis=1), X1[\"Target\"])\n",
    "dtrain2 = xgb.DMatrix(X2.drop([\"Target\"], axis=1), X2[\"Target\"])\n",
    "dtrain3 = xgb.DMatrix(X.drop([\"Target\"], axis=1), X[\"Target\"])\n",
    "\n",
    "dtest = xgb.DMatrix(leaderboard)\n",
    "cv_result = xgb.cv(xgb_params, \n",
    "                   dtrain1, \n",
    "                   num_boost_round=1200, # increase to have better results (~700)\n",
    "                   verbose_eval=50\n",
    "                  )\n",
    "num_boost_rounds = len(cv_result)\n",
    "print('num_boost_rounds=' + str(num_boost_rounds))\n",
    "\n",
    "cv_result = xgb.cv(xgb_params, \n",
    "                   dtrain2, \n",
    "                   num_boost_round=1200, # increase to have better results (~700)\n",
    "                   verbose_eval=50\n",
    "                  )\n",
    "num_boost_rounds = len(cv_result)\n",
    "print('num_boost_rounds=' + str(num_boost_rounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30159    94]\n",
      " [ 3658    89]]\n",
      "[[ 5276  6805]\n",
      " [ 8008 13911]]\n"
     ]
    }
   ],
   "source": [
    "lasso = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "y_pred = cross_val_predict(lasso, X1.drop([\"Target\"], axis=1), X1[\"Target\"], cv=3)\n",
    "print(confusion_matrix(X1[\"Target\"], y_pred))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, X2.drop([\"Target\"], axis=1), X2[\"Target\"], cv=3)\n",
    "print(confusion_matrix(X2[\"Target\"], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    18172\n",
      "1     3747\n",
      "Name: Target, dtype: int64 [18172 18172]\n",
      "[[15249  2923]\n",
      " [ 2997   750]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Not       0.84      0.84      0.84     18172\n",
      "        High       0.20      0.20      0.20      3747\n",
      "\n",
      "    accuracy                           0.73     21919\n",
      "   macro avg       0.52      0.52      0.52     21919\n",
      "weighted avg       0.73      0.73      0.73     21919\n",
      "\n",
      "[[  598 17574]\n",
      " [ 2901 15271]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Not       0.17      0.03      0.06     18172\n",
      "        High       0.46      0.84      0.60     18172\n",
      "\n",
      "    accuracy                           0.44     36344\n",
      "   macro avg       0.32      0.44      0.33     36344\n",
      "weighted avg       0.32      0.44      0.33     36344\n",
      "\n",
      "1    18172\n",
      "0    12081\n",
      "Name: Target, dtype: int64 [18172 18172]\n",
      "[[ 5673  6408]\n",
      " [ 7021 11151]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Not       0.45      0.47      0.46     12081\n",
      "         Low       0.64      0.61      0.62     18172\n",
      "\n",
      "    accuracy                           0.56     30253\n",
      "   macro avg       0.54      0.54      0.54     30253\n",
      "weighted avg       0.56      0.56      0.56     30253\n",
      "\n",
      "[[10808  7364]\n",
      " [18165     7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Not       0.37      0.59      0.46     18172\n",
      "         Low       0.00      0.00      0.00     18172\n",
      "\n",
      "    accuracy                           0.30     36344\n",
      "   macro avg       0.19      0.30      0.23     36344\n",
      "weighted avg       0.19      0.30      0.23     36344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "sm = SMOTE(random_state=12)\n",
    "x_train_res, y_train_res = sm.fit_sample(X1.drop([\"Target\"], axis=1), X1[\"Target\"])\n",
    "print (X1[\"Target\"].value_counts() , np.bincount(y_train_res))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, X1.drop([\"Target\"], axis=1), X1[\"Target\"], cv=3)\n",
    "print(confusion_matrix(X1[\"Target\"], y_pred))\n",
    "print(classification_report(X1[\"Target\"], y_pred, target_names=[\"Not\", \"High\"]))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, x_train_res, y_train_res, cv=3)\n",
    "print(confusion_matrix(y_train_res, y_pred))\n",
    "print(classification_report(y_train_res, y_pred, target_names=[\"Not\", \"High\"]))\n",
    "\n",
    "x_train_res, y_train_res = sm.fit_sample(X2.drop([\"Target\"], axis=1), X2[\"Target\"])\n",
    "print (X2[\"Target\"].value_counts() , np.bincount(y_train_res))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, X2.drop([\"Target\"], axis=1), X2[\"Target\"], cv=3)\n",
    "print(confusion_matrix(X2[\"Target\"], y_pred))\n",
    "print(classification_report(X2[\"Target\"], y_pred, target_names=[\"Not\", \"Low\"]))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, x_train_res, y_train_res, cv=3)\n",
    "print(confusion_matrix(y_train_res, y_pred))\n",
    "print(classification_report(y_train_res, y_pred, target_names=[\"Not\", \"Low\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    18172\n",
      "1     3747\n",
      "Name: Target, dtype: int64 [18172 18172]\n",
      "[[16344  1828]\n",
      " [ 3174   573]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Not       0.84      0.90      0.87     18172\n",
      "        High       0.24      0.15      0.19      3747\n",
      "\n",
      "    accuracy                           0.77     21919\n",
      "   macro avg       0.54      0.53      0.53     21919\n",
      "weighted avg       0.74      0.77      0.75     21919\n",
      "\n",
      "[[  639 17533]\n",
      " [ 2907 15265]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Not       0.18      0.04      0.06     18172\n",
      "        High       0.47      0.84      0.60     18172\n",
      "\n",
      "    accuracy                           0.44     36344\n",
      "   macro avg       0.32      0.44      0.33     36344\n",
      "weighted avg       0.32      0.44      0.33     36344\n",
      "\n",
      "1    18172\n",
      "0    12081\n",
      "Name: Target, dtype: int64 [18172 18172]\n",
      "[[ 5799  6282]\n",
      " [ 6900 11272]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Not       0.46      0.48      0.47     12081\n",
      "         Low       0.64      0.62      0.63     18172\n",
      "\n",
      "    accuracy                           0.56     30253\n",
      "   macro avg       0.55      0.55      0.55     30253\n",
      "weighted avg       0.57      0.56      0.57     30253\n",
      "\n",
      "[[10597  7575]\n",
      " [18157    15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Not       0.37      0.58      0.45     18172\n",
      "         Low       0.00      0.00      0.00     18172\n",
      "\n",
      "    accuracy                           0.29     36344\n",
      "   macro avg       0.19      0.29      0.23     36344\n",
      "weighted avg       0.19      0.29      0.23     36344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "sm = BorderlineSMOTE(random_state=12)\n",
    "x_train_res, y_train_res = sm.fit_sample(X1.drop([\"Target\"], axis=1), X1[\"Target\"])\n",
    "print (X1[\"Target\"].value_counts() , np.bincount(y_train_res))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, X1.drop([\"Target\"], axis=1), X1[\"Target\"], cv=3)\n",
    "print(confusion_matrix(X1[\"Target\"], y_pred))\n",
    "print(classification_report(X1[\"Target\"], y_pred, target_names=[\"Not\", \"High\"]))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, x_train_res, y_train_res, cv=3)\n",
    "print(confusion_matrix(y_train_res, y_pred))\n",
    "print(classification_report(y_train_res, y_pred, target_names=[\"Not\", \"High\"]))\n",
    "\n",
    "x_train_res, y_train_res = sm.fit_sample(X2.drop([\"Target\"], axis=1), X2[\"Target\"])\n",
    "print (X2[\"Target\"].value_counts() , np.bincount(y_train_res))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, X2.drop([\"Target\"], axis=1), X2[\"Target\"], cv=3)\n",
    "print(confusion_matrix(X2[\"Target\"], y_pred))\n",
    "print(classification_report(X2[\"Target\"], y_pred, target_names=[\"Not\", \"Low\"]))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, x_train_res, y_train_res, cv=3)\n",
    "print(confusion_matrix(y_train_res, y_pred))\n",
    "print(classification_report(y_train_res, y_pred, target_names=[\"Not\", \"Low\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    18172\n",
      "1     3747\n",
      "Name: Target, dtype: int64 [18172 19347]\n",
      "[[15372  2800]\n",
      " [ 2988   759]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Not       0.84      0.85      0.84     18172\n",
      "        High       0.21      0.20      0.21      3747\n",
      "\n",
      "    accuracy                           0.74     21919\n",
      "   macro avg       0.53      0.52      0.52     21919\n",
      "weighted avg       0.73      0.74      0.73     21919\n",
      "\n",
      "[[  607 17565]\n",
      " [ 2812 16535]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Not       0.18      0.03      0.06     18172\n",
      "        High       0.48      0.85      0.62     19347\n",
      "\n",
      "    accuracy                           0.46     37519\n",
      "   macro avg       0.33      0.44      0.34     37519\n",
      "weighted avg       0.34      0.46      0.35     37519\n",
      "\n",
      "1    18172\n",
      "0    12081\n",
      "Name: Target, dtype: int64 [20031 18172]\n",
      "[[ 5796  6285]\n",
      " [ 7150 11022]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Not       0.45      0.48      0.46     12081\n",
      "         Low       0.64      0.61      0.62     18172\n",
      "\n",
      "    accuracy                           0.56     30253\n",
      "   macro avg       0.54      0.54      0.54     30253\n",
      "weighted avg       0.56      0.56      0.56     30253\n",
      "\n",
      "[[12583  7448]\n",
      " [18155    17]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Not       0.41      0.63      0.50     20031\n",
      "         Low       0.00      0.00      0.00     18172\n",
      "\n",
      "    accuracy                           0.33     38203\n",
      "   macro avg       0.21      0.31      0.25     38203\n",
      "weighted avg       0.22      0.33      0.26     38203\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "sm = ADASYN(random_state=12)\n",
    "x_train_res, y_train_res = sm.fit_sample(X1.drop([\"Target\"], axis=1), X1[\"Target\"])\n",
    "print (X1[\"Target\"].value_counts() , np.bincount(y_train_res))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, X1.drop([\"Target\"], axis=1), X1[\"Target\"], cv=3)\n",
    "print(confusion_matrix(X1[\"Target\"], y_pred))\n",
    "print(classification_report(X1[\"Target\"], y_pred, target_names=[\"Not\", \"High\"]))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, x_train_res, y_train_res, cv=3)\n",
    "print(confusion_matrix(y_train_res, y_pred))\n",
    "print(classification_report(y_train_res, y_pred, target_names=[\"Not\", \"High\"]))\n",
    "\n",
    "x_train_res, y_train_res = sm.fit_sample(X2.drop([\"Target\"], axis=1), X2[\"Target\"])\n",
    "print (X2[\"Target\"].value_counts() , np.bincount(y_train_res))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, X2.drop([\"Target\"], axis=1), X2[\"Target\"], cv=3)\n",
    "print(confusion_matrix(X2[\"Target\"], y_pred))\n",
    "print(classification_report(X2[\"Target\"], y_pred, target_names=[\"Not\", \"Low\"]))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, x_train_res, y_train_res, cv=3)\n",
    "print(confusion_matrix(y_train_res, y_pred))\n",
    "print(classification_report(y_train_res, y_pred, target_names=[\"Not\", \"Low\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No clusters found with sufficient samples of class 1. Try lowering the cluster_balance_threshold or increasing the number of clusters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-06f9b95ff74e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeansSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mx_train_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     82\u001b[0m             self.sampling_strategy, y, self._sampling_type)\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbinarize_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/imblearn/over_sampling/_smote.py\u001b[0m in \u001b[0;36m_fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[0;31m# FIXME: rename _sample -> _fit_resample in 0.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/imblearn/over_sampling/_smote.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                     \u001b[0;34m\"class {}. Try lowering the cluster_balance_threshold \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                     \u001b[0;34m\"or increasing the number of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                     \"clusters.\".format(class_sample))\n\u001b[0m\u001b[1;32m   1296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mvalid_cluster_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_cluster\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No clusters found with sufficient samples of class 1. Try lowering the cluster_balance_threshold or increasing the number of clusters."
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "sm = KMeansSMOTE(random_state=12)\n",
    "x_train_res, y_train_res = sm.fit_sample(X1.drop([\"Target\"], axis=1), X1[\"Target\"])\n",
    "print (X1[\"Target\"].value_counts() , np.bincount(y_train_res))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, X1.drop([\"Target\"], axis=1), X1[\"Target\"], cv=3)\n",
    "print(confusion_matrix(X1[\"Target\"], y_pred))\n",
    "print(classification_report(X1[\"Target\"], y_pred, target_names=[\"Not\", \"High\"]))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, x_train_res, y_train_res, cv=3)\n",
    "print(confusion_matrix(y_train_res, y_pred))\n",
    "print(classification_report(y_train_res, y_pred, target_names=[\"Not\", \"High\"]))\n",
    "\n",
    "x_train_res, y_train_res = sm.fit_sample(X2.drop([\"Target\"], axis=1), X2[\"Target\"])\n",
    "print (X2[\"Target\"].value_counts() , np.bincount(y_train_res))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, X2.drop([\"Target\"], axis=1), X2[\"Target\"], cv=3)\n",
    "print(confusion_matrix(X2[\"Target\"], y_pred))\n",
    "print(classification_report(X2[\"Target\"], y_pred, target_names=[\"Not\", \"Low\"]))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, x_train_res, y_train_res, cv=3)\n",
    "print(confusion_matrix(y_train_res, y_pred))\n",
    "print(classification_report(y_train_res, y_pred, target_names=[\"Not\", \"Low\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    18172\n",
      "1     3747\n",
      "Name: Target, dtype: int64 [18172 18172]\n",
      "[[15323  2849]\n",
      " [ 3033   714]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Not       0.83      0.84      0.84     18172\n",
      "        High       0.20      0.19      0.20      3747\n",
      "\n",
      "    accuracy                           0.73     21919\n",
      "   macro avg       0.52      0.52      0.52     21919\n",
      "weighted avg       0.73      0.73      0.73     21919\n",
      "\n",
      "[[  174 17998]\n",
      " [  172 18000]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Not       0.50      0.01      0.02     18172\n",
      "        High       0.50      0.99      0.66     18172\n",
      "\n",
      "    accuracy                           0.50     36344\n",
      "   macro avg       0.50      0.50      0.34     36344\n",
      "weighted avg       0.50      0.50      0.34     36344\n",
      "\n",
      "1    18172\n",
      "0    12081\n",
      "Name: Target, dtype: int64 [18172 18172]\n",
      "[[ 5806  6275]\n",
      " [ 6822 11350]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Not       0.46      0.48      0.47     12081\n",
      "         Low       0.64      0.62      0.63     18172\n",
      "\n",
      "    accuracy                           0.57     30253\n",
      "   macro avg       0.55      0.55      0.55     30253\n",
      "weighted avg       0.57      0.57      0.57     30253\n",
      "\n",
      "[[14595  3577]\n",
      " [18159    13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Not       0.45      0.80      0.57     18172\n",
      "         Low       0.00      0.00      0.00     18172\n",
      "\n",
      "    accuracy                           0.40     36344\n",
      "   macro avg       0.22      0.40      0.29     36344\n",
      "weighted avg       0.22      0.40      0.29     36344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "sm = RandomOverSampler(random_state=12)\n",
    "x_train_res, y_train_res = sm.fit_sample(X1.drop([\"Target\"], axis=1), X1[\"Target\"])\n",
    "print (X1[\"Target\"].value_counts() , np.bincount(y_train_res))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, X1.drop([\"Target\"], axis=1), X1[\"Target\"], cv=3)\n",
    "print(confusion_matrix(X1[\"Target\"], y_pred))\n",
    "print(classification_report(X1[\"Target\"], y_pred, target_names=[\"Not\", \"High\"]))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, x_train_res, y_train_res, cv=3)\n",
    "print(confusion_matrix(y_train_res, y_pred))\n",
    "print(classification_report(y_train_res, y_pred, target_names=[\"Not\", \"High\"]))\n",
    "\n",
    "x_train_res, y_train_res = sm.fit_sample(X2.drop([\"Target\"], axis=1), X2[\"Target\"])\n",
    "print (X2[\"Target\"].value_counts() , np.bincount(y_train_res))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, X2.drop([\"Target\"], axis=1), X2[\"Target\"], cv=3)\n",
    "print(confusion_matrix(X2[\"Target\"], y_pred))\n",
    "print(classification_report(X2[\"Target\"], y_pred, target_names=[\"Not\", \"Low\"]))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, x_train_res, y_train_res, cv=3)\n",
    "print(confusion_matrix(y_train_res, y_pred))\n",
    "print(classification_report(y_train_res, y_pred, target_names=[\"Not\", \"Low\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-77e3b51cd016>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTENC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mx_train_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     82\u001b[0m             self.sampling_strategy, y, self._sampling_type)\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbinarize_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/imblearn/over_sampling/_smote.py\u001b[0m in \u001b[0;36m_fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    986\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0;31m# compute the median of the standard deviation of the minority class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/imblearn/over_sampling/_smote.py\u001b[0m in \u001b[0;36m_validate_estimator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m             if any([cat not in np.arange(self.n_features_)\n\u001b[0;32m--> 978\u001b[0;31m                     for cat in categorical_features]):\n\u001b[0m\u001b[1;32m    979\u001b[0m                 raise ValueError(\n\u001b[1;32m    980\u001b[0m                     \u001b[0;34m'Some of the categorical indices are out of range. Indices'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d array"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "sm = SMOTENC(random_state=12, categorical_features=None)\n",
    "x_train_res, y_train_res = sm.fit_sample(X1.drop([\"Target\"], axis=1), X1[\"Target\"])\n",
    "print (X1[\"Target\"].value_counts() , np.bincount(y_train_res))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, X1.drop([\"Target\"], axis=1), X1[\"Target\"], cv=3)\n",
    "print(confusion_matrix(X1[\"Target\"], y_pred))\n",
    "print(classification_report(X1[\"Target\"], y_pred, target_names=[\"Not\", \"High\"]))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, x_train_res, y_train_res, cv=3)\n",
    "print(confusion_matrix(y_train_res, y_pred))\n",
    "print(classification_report(y_train_res, y_pred, target_names=[\"Not\", \"High\"]))\n",
    "\n",
    "x_train_res, y_train_res = sm.fit_sample(X2.drop([\"Target\"], axis=1), X2[\"Target\"])\n",
    "print (X2[\"Target\"].value_counts() , np.bincount(y_train_res))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, X2.drop([\"Target\"], axis=1), X2[\"Target\"], cv=3)\n",
    "print(confusion_matrix(X2[\"Target\"], y_pred))\n",
    "print(classification_report(X2[\"Target\"], y_pred, target_names=[\"Not\", \"Low\"]))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, x_train_res, y_train_res, cv=3)\n",
    "print(confusion_matrix(y_train_res, y_pred))\n",
    "print(classification_report(y_train_res, y_pred, target_names=[\"Not\", \"Low\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    18172\n",
      "1     3747\n",
      "Name: Target, dtype: int64 [18172 18172]\n",
      "[[14956  3216]\n",
      " [ 2950   797]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Not       0.84      0.82      0.83     18172\n",
      "        High       0.20      0.21      0.21      3747\n",
      "\n",
      "    accuracy                           0.72     21919\n",
      "   macro avg       0.52      0.52      0.52     21919\n",
      "weighted avg       0.73      0.72      0.72     21919\n",
      "\n",
      "[[   11 18161]\n",
      " [ 5185 12987]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Not       0.00      0.00      0.00     18172\n",
      "        High       0.42      0.71      0.53     18172\n",
      "\n",
      "    accuracy                           0.36     36344\n",
      "   macro avg       0.21      0.36      0.26     36344\n",
      "weighted avg       0.21      0.36      0.26     36344\n",
      "\n",
      "1    18172\n",
      "0    12081\n",
      "Name: Target, dtype: int64 [18172 18172]\n",
      "[[ 5868  6213]\n",
      " [ 6943 11229]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Not       0.46      0.49      0.47     12081\n",
      "         Low       0.64      0.62      0.63     18172\n",
      "\n",
      "    accuracy                           0.57     30253\n",
      "   macro avg       0.55      0.55      0.55     30253\n",
      "weighted avg       0.57      0.57      0.57     30253\n",
      "\n",
      "[[10819  7353]\n",
      " [18169     3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Not       0.37      0.60      0.46     18172\n",
      "         Low       0.00      0.00      0.00     18172\n",
      "\n",
      "    accuracy                           0.30     36344\n",
      "   macro avg       0.19      0.30      0.23     36344\n",
      "weighted avg       0.19      0.30      0.23     36344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "sm = SVMSMOTE(random_state=12)\n",
    "x_train_res, y_train_res = sm.fit_sample(X1.drop([\"Target\"], axis=1), X1[\"Target\"])\n",
    "print (X1[\"Target\"].value_counts() , np.bincount(y_train_res))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, X1.drop([\"Target\"], axis=1), X1[\"Target\"], cv=3)\n",
    "print(confusion_matrix(X1[\"Target\"], y_pred))\n",
    "print(classification_report(X1[\"Target\"], y_pred, target_names=[\"Not\", \"High\"]))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, x_train_res, y_train_res, cv=3)\n",
    "print(confusion_matrix(y_train_res, y_pred))\n",
    "print(classification_report(y_train_res, y_pred, target_names=[\"Not\", \"High\"]))\n",
    "\n",
    "x_train_res, y_train_res = sm.fit_sample(X2.drop([\"Target\"], axis=1), X2[\"Target\"])\n",
    "print (X2[\"Target\"].value_counts() , np.bincount(y_train_res))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, X2.drop([\"Target\"], axis=1), X2[\"Target\"], cv=3)\n",
    "print(confusion_matrix(X2[\"Target\"], y_pred))\n",
    "print(classification_report(X2[\"Target\"], y_pred, target_names=[\"Not\", \"Low\"]))\n",
    "\n",
    "y_pred = cross_val_predict(lasso, x_train_res, y_train_res, cv=3)\n",
    "print(confusion_matrix(y_train_res, y_pred))\n",
    "print(classification_report(y_train_res, y_pred, target_names=[\"Not\", \"Low\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1297aa0f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD4CAYAAAD2FnFTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAATqUlEQVR4nO3dX7Bd5X3e8e9TuUBkbEU2lOmBWMcQ\nio0sS1DV0ykB49AWMDgkJEwOTTOeRh0mrTsehrqDM2YytL6hLb3oBUHDeBi7F5VMIAJPMg6hiQin\ndWw4SgSCGhyMyCRqCQ1yFGNLOIZfL/ZSWGy96PzZR2efDd/PzB6t9a537f1bC7Eevevdf1JVSJI0\n7G+NuwBJ0upkQEiSmgwISVKTASFJajIgJElN7xh3AaM67bTTanp6etxlSNLE2LNnz19U1enz9Zv4\ngJienmZubm7cZUjSxEjyJwvp5y0mSVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLUNPGfg/jz\n557lv/z81eMu44T4t1/+zXGXIOltzBGEJKnJgJAkNRkQkqQmA0KS1GRASJKalhwQSXYnuXyo7cYk\nd/aWjyRZ19t+aZJDSfYmeTrJ7b1tv5DkiST7knwtyeal1iZJGt0oI4gdwMxQ20zXDnA98Bhw7VCf\n2araAlwAXJ3koq59P/DRqtoEfB64a4TaJEkjGiUg7gWuSnISQJJpYAqYTXIOcCpwC4OgOEZVHQb2\nAmd261+rqu90m78OnDVCbZKkES05IKrqIPAocGXXNAPcU1XVLe8EZoHzkpwxvH+S9cC5wCONp98G\nfPXNXjvJDUnmksx975UfLPUQJEnHMeokdf820/DtpZ1V9RpwH3Bdb5+LkzwOHAAerKoX+k+Y5GMM\nAuLmN3vRqrqrqrZW1dZ3nnzSiIcgSWoZNSAeAC5LciGwtqr2JNnEYGTwUJLnGQRH/zbTbFVtBjYC\n25JsObohyYeBLwDXVNVLI9YmSRrBSAFRVS8Du4G7eePo4daqmu4eU8BUkg1D++4HbqMbKSR5H/Ab\nwC9W1bdGqUuSNLrl+BzEDmAzrwfEDLBrqM8ujn3HE8B24JJugvtXgfcCv9a9DXZuGWqTJC3RyN/m\nWlX3A+mtn93oc1Nv9eFe+2G6dzEB/7J7SJJWAT9JLUlqMiAkSU0GhCSpyYCQJDVN/E+OnnH2j/vT\nnJJ0AjiCkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRA\nSJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDVN/C/Kvfgn3+WOX/69cZdxwnxq+0+OuwRJb1OOICRJ\nTQaEJKnJgJAkNRkQkqQmA0KS1LTkgEiyO8nlQ203Jrmzt3wkybre9kuTHEqyN8nTSW7vbft3Xfve\nJE8meTXJe5ZanyRpNKOMIHYAM0NtM107wPXAY8C1Q31mq2oLcAFwdZKLAKrqP1fVlm7brwC/X1UH\nR6hPkjSCUQLiXuCqJCcBJJkGpoDZJOcApwK3MAiKY1TVYWAvcGZj8/W8HjSSpDFYckB0/7p/FLiy\na5oB7qmq6pZ3ArPAeUnOGN4/yXrgXOCRofa1wBXAfW/22kluSDKXZO7lI3+51EOQJB3HqJPU/dtM\nw7eXdlbVawwu9Nf19rk4yePAAeDBqnph6Dk/Afyv491eqqq7qmprVW099ZQfHfEQJEktowbEA8Bl\nSS4E1lbVniSbGIwMHkryPIPg6N9mmq2qzcBGYFuSLUPP2Q8aSdKYjBQQVfUysBu4mzeOHm6tqunu\nMQVMJdkwtO9+4Dbg5qNt3TuePsogeCRJY7Qcn4PYAWzm9YCYAXYN9dnFse94AtgOXNJNcAP8DPA7\nVfW9ZahLkjSCkb/NtaruB9JbP7vR56be6sO99sP03sVUVV8EvjhqTZKk0flJaklSkwEhSWoyICRJ\nTRP/i3J/Z8O7/NU1SToBHEFIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKa\nDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmib+F+WOPPkU3/zAB8ddxor44NPf\nHHcJkt5GHEFIkpoMCElSkwEhSWoyICRJTQaEJKnpuAGRZHeSy4fabkxyZ2/5SJJ1ve2XJjmUZG+S\np5Pc3tv2gSR/kOSVJJ/ptZ+S5NEkjyd5Ksm/X75DlCQtxXwjiB3AzFDbTNcOcD3wGHDtUJ/ZqtoC\nXABcneSirv0g8Gng9qH+rwA/WVWbgS3AFUn+4YKPQpK07OYLiHuBq5KcBJBkGpgCZpOcA5wK3MIg\nKI5RVYeBvcCZ3fqLVfUY8NdD/aqqXu5W/3b3qCUcjyRpmRw3IKrqIPAocGXXNAPcU1XVLe8EZoHz\nkpwxvH+S9cC5wCPzFZJkTZK9wIvAQ1X1jeP0vSHJXJK5g6/+cL6nliQtwUImqfu3mYZvL+2sqteA\n+4DrevtcnORx4ADwYFW9MN+LVNWr3W2ps4CPJPnQcfreVVVbq2rre9ZM/IfBJWlVWkhAPABcluRC\nYG1V7UmyicHI4KEkzzMIjv5tptluPmEjsC3JloUWVFV/CewGrljoPpKk5TdvQHRzA7uBu3nj6OHW\nqpruHlPAVJINQ/vuB24Dbj7eayQ5PcmPdss/AvwT4OnFHowkafks9P7MDmAXb7zV9PGhPke3D88d\nbAc+001wHwHmgHcDryW5ETgf+LvAl5KsYRBa91TVby7qSCRJyyqD+ebJ9aFTfqR+fXp63GWsCL/N\nVdJySLKnqrbO189PUkuSmgwISVKTASFJapr4DxGc8qGNfHBubtxlSNJbjiMISVKTASFJajIgJElN\nBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRA\nSJKaDAhJUtPE/6LcUy89xaYvbRp3GW8r+z65b9wlSFoBjiAkSU0GhCSpyYCQJDUZEJKkJgNCktR0\n3IBIsjvJ5UNtNya5s7d8JMm63vZLkxxKsjfJ00lu7237QJI/SPJKks8MPe/zSfZ1+80tz+FJkpZq\nvhHEDmBmqG2mawe4HngMuHaoz2xVbQEuAK5OclHXfhD4NHA7bR+rqi1VtXUhxUuSTpz5AuJe4Kok\nJwEkmQamgNkk5wCnArcwCIpjVNVhYC9wZrf+YlU9Bvz1chQvSTpxjhsQVXUQeBS4smuaAe6pquqW\ndwKzwHlJzhjeP8l64FzgkQXUUsDvJNmT5IbjdUxyQ5K5JHOvfvfVBTy1JGmxFjJJ3b/NNHx7aWdV\nvQbcB1zX2+fiJI8DB4AHq+qFBbzOT1TVhQzC6FNJLnmzjlV1V1Vtraqta961ZgFPLUlarIUExAPA\nZUkuBNZW1Z4kmxiMDB5K8jyD4OjfZpqtqs3ARmBbki3zvUhVHej+fBHYBXxkUUciSVpW8wZEVb0M\n7Abu5o2jh1urarp7TAFTSTYM7bsfuA24+XivkeSdSd51dBn4p8CTiz0YSdLyWeiX9e1g8K/6/q2m\njw/1Obr9G0Pt24HPdBPcR4A54N3Aa0luBM4HTgN2JTla03+vqt9ezIFIkpbXggKiqu4H0ls/u9Hn\npt7qw732w3TvYuqc1XiJvwI2L6QWSdLK8JPUkqQmA0KS1GRASJKaJv4X5Ta+dyNzn/SrmyRpuTmC\nkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJ\nUpMBIUlqMiAkSU0GhCSpyYCQJDVN/C/K8X/+CG5dN+4qpMW59dC4K5Dm5QhCktRkQEiSmgwISVKT\nASFJajIgJElNxw2IJLuTXD7UdmOSO3vLR5Ks622/NMmhJHuTPJ3k9t62X0jyRJJ9Sb6WZHNv2xVJ\nnknybJLPLt8hSpKWYr4RxA5gZqhtpmsHuB54DLh2qM9sVW0BLgCuTnJR174f+GhVbQI+D9wFkGQN\ncAdwJXA+cH2S8xd/OJKk5TJfQNwLXJXkJIAk08AUMJvkHOBU4BYGQXGMqjoM7AXO7Na/VlXf6TZ/\nHTirW/4I8GxVPVdVPwB2Atcs8ZgkScvguAFRVQeBRxn8yx4Go4d7qqq65Z3ALHBekjOG90+yHjgX\neKTx9NuAr3bLZwJ/2tv2Z11bU5Ibkswlmft/36/jHYIkaYkWMkndv800fHtpZ1W9BtwHXNfb5+Ik\njwMHgAer6oX+Eyb5GIOAuHkpRVfVXVW1taq2nr42S3kKSdI8FhIQDwCXJbkQWFtVe5JsYjAyeCjJ\n8wyCo3+babaqNgMbgW1JthzdkOTDwBeAa6rqpa75APBjvf3P6tokSWMyb0BU1cvAbuBu3jh6uLWq\nprvHFDCVZMPQvvuB2+hGCkneB/wG8ItV9a1e18eAc5O8v5vvmAG+MtqhSZJGsdDPQewANvN6QMwA\nu4b67OLYdzwBbAcu6Sa4fxV4L/Br3dtg5wCq6ofAvwEeBL7JYJ7jqYUfhiRpuWUw3zy5tk6tqbkb\nTh13GdLi+G2uGqMke6pq63z9/CS1JKnJgJAkNRkQkqQmA0KS1DT5Pzk6dQHcOjfuKiTpLccRhCSp\nyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoM\nCElSkwEhSWoyICRJTQaEJKlp4n9Rbt+BQ0x/9rfGXYYkrZjnb7tqRV7HEYQkqcmAkCQ1GRCSpCYD\nQpLUZEBIkpqWHBBJdie5fKjtxiR39paPJFnX235pkkNJ9iZ5OsntvW3XJHmi2zaX5CeWWpskaXSj\njCB2ADNDbTNdO8D1wGPAtUN9ZqtqC3ABcHWSi7r23wU2d9t+CfjCCLVJkkY0SkDcC1yV5CSAJNPA\nFDCb5BzgVOAWBkFxjKo6DOwFzuzWX66q6ja/E6jWfpKklbHkgKiqg8CjwJVd0wxwT3eRnwF2ArPA\neUnOGN4/yXrgXOCRXtvPJHka+C0Go4imJDd0t6HmXv3+oaUegiTpOEadpO7fZhq+vbSzql4D7gOu\n6+1zcZLHgQPAg1X1wtENVbWrqj4A/DTw+Td70aq6q6q2VtXWNWvXvVk3SdIIRg2IB4DLklwIrK2q\nPUk2MRgZPJTkeQbB0b/NNFtVm4GNwLYkW4aftKoeAc5OctqI9UmSlmikgKiql4HdwN28cfRwa1VN\nd48pYCrJhqF99wO3ATcDJPnxJOmWLwROBl4apT5J0tItx+cgdgCbeT0gZoBdQ312cew7ngC2A5d0\nE9w/CzyZZC9wB/DzvUlrSdIKG/nbXKvqfiC99bMbfW7qrT7caz9M9y4m4D92D0nSKuAnqSVJTQaE\nJKnJgJAkNU38L8ptOnMdcyv060qS9HbiCEKS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZ\nEJKkpkz6F6Ym+S7wzLjrWILTgL8YdxFLZO3jMam1T2rd8NatfUNVnT7fE0z8J6mBZ6pq67iLWKwk\nc5NYN1j7uExq7ZNaN1i7t5gkSU0GhCSp6a0QEHeNu4AlmtS6wdrHZVJrn9S64W1e+8RPUkuSToy3\nwghCknQCGBCSpKZVGxBJrkjyTJJnk3y2sf3kJF/utn8jyXRv26907c8kuXwl6+5ef0m1J5lOcjjJ\n3u6xfRXWfkmSP0zywyQ/N7Ttk0n+uHt8cuWqHrnuV3vn/CsrV/XfvP58td+U5H8neSLJ7ybZ0Ns2\ntnPevf4ota/28/7LSfZ19f3PJOf3to3tGrPUupd0famqVfcA1gDfBs4GTgIeB84f6vOvge3d8gzw\n5W75/K7/ycD7u+dZMyG1TwNPrvLzPg18GPhvwM/12t8DPNf9ub5bXr/a6+62vbzKz/nHgLXd8r/q\n/X0Z2zkftfYJOe/v7i3/FPDb3fLYrjEj1r3o68tqHUF8BHi2qp6rqh8AO4FrhvpcA3ypW74XuCxJ\nuvadVfVKVe0Hnu2eb6WMUvu4zVt7VT1fVU8Arw3teznwUFUdrKrvAA8BV6xE0YxW97gtpPbdVfX9\nbvXrwFnd8jjPOYxW+7gtpPa/6q2+Ezj6jp5xXmNGqXvRVmtAnAn8aW/9z7q2Zp+q+iFwCHjvAvc9\nkUapHeD9Sf4oye8nufhEF/tmdXUWc+7Ged5Hfe1Tkswl+XqSn17e0ua12Nq3AV9d4r7LbZTaYQLO\ne5JPJfk28J+ATy9m3xNklLphkdeXt8JXbbyV/F/gfVX1UpK/D9yfZOPQvwi0/DZU1YEkZwO/l2Rf\nVX173EUNS/LPga3AR8ddy2K9Se2r/rxX1R3AHUn+GXALsOLzPEvxJnUv+vqyWkcQB4Af662f1bU1\n+yR5B7AOeGmB+55IS669G7K+BFBVexjca/x7J7ziRl2dxZy7cZ73kV67qg50fz4HPAxcsJzFzWNB\ntSf5x8DngJ+qqlcWs+8JNErtE3Hee3YCR0c5k/R3/W/qXtL1ZSUmVpYwEfMOBhNu7+f1iZiNQ30+\nxRsneu/pljfyxgmk51jZSepRaj/9aK0MJqEOAO9ZTbX3+n6RYyep9zOYLF3fLa9I7SPWvR44uVs+\nDfhjhib9xl07gwvnt4Fzh9rHds6XofZJOO/n9pY/Acx1y2O7xoxY96KvLyvyH2OJJ+LjwLe6v1yf\n69r+A4N/hQCcAvw6gwmiR4Gze/t+rtvvGeDKSakd+FngKWAv8IfAJ1Zh7f+AwX3P7zEYsT3V2/eX\numN6FvgXk1A38I+Afd3/aPuAbavwnP8P4M+7vxd7ga+shnM+Su0Tct7/a+//x930LsTjvMYste6l\nXF/8qg1JUtNqnYOQJI2ZASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLU9P8BZ8w4FSWtQZsAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "clf = BalancedRandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X.drop([\"Target\"], axis=1), X[\"Target\"])\n",
    "clf.feature_importances_\n",
    "\n",
    "(pd.Series(clf.feature_importances_, index=X.drop([\"Target\"], axis=1).columns)\n",
    "   .nlargest(6)\n",
    "   .plot(kind='barh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:530: FutureWarning: From version 0.22, errors during fit will result in a cross validation score of NaN by default. Use error_score='raise' if you want an exception raised or error_score=np.nan to adopt the behavior from version 0.22.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'prefer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-d1c2c6e39416>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBalancedRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    389\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    392\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 232\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib-0.11-py3.6.egg/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib-0.11-py3.6.egg/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib-0.11-py3.6.egg/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib-0.11-py3.6.egg/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib-0.11-py3.6.egg/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib-0.11-py3.6.egg/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib-0.11-py3.6.egg/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/imblearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;31m# threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             samplers_trees = Parallel(\n\u001b[0;32m--> 431\u001b[0;31m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"threads\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                     delayed(_local_parallel_build_trees)(\n\u001b[1;32m    433\u001b[0m                         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'prefer'"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = BalancedRandomForestClassifier(max_depth=2, random_state=0)\n",
    "\n",
    "print(cross_val_score(clf, X1.drop([\"Target\"], axis=1), X1[\"Target\"], cv=3))\n",
    "\n",
    "y_pred = cross_val_predict(clf, X1.drop([\"Target\"], axis=1), X1[\"Target\"], cv=3)\n",
    "print(confusion_matrix(X1[\"Target\"], y_pred))\n",
    "print(classification_report(X1[\"Target\"], y_pred, target_names=[\"Not\", \"High\"]))\n",
    "\n",
    "y_pred = cross_val_predict(clf, X2.drop([\"Target\"], axis=1), X2[\"Target\"], cv=3)\n",
    "print(confusion_matrix(X2[\"Target\"], y_pred))\n",
    "print(classification_report(X2[\"Target\"], y_pred, target_names=[\"Not\", \"Low\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# High\n",
    "X[\"Target\"][X[\"Target\"] == 0] = 3\n",
    "# Low\n",
    "X[\"Target\"][X[\"Target\"] == 1] = 0\n",
    "# Medium\n",
    "X[\"Target\"][X[\"Target\"] == 2] = 1\n",
    "# High\n",
    "X[\"Target\"][X[\"Target\"] == 3] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X1, X_val, y1, y_val = train_test_split(X, y,\n",
    "                                                    train_size=30000, test_size=4000)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "X1[\"Target\"] = y1\n",
    "stack = X1[[\"Target\"]]\n",
    "stack[\"1\"]=0\n",
    "stack[\"2\"]=0\n",
    "stack[\"3\"]=0\n",
    "stack[\"4\"]=0\n",
    "stack[\"5\"]=0\n",
    "stack[\"6\"]=0\n",
    "stack[\"7\"]=0\n",
    "stack[\"8\"]=0\n",
    "stack[\"9\"]=0\n",
    "X1 = X1.drop([\"Target\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [10000 10001 10002 ... 29997 29998 29999] TEST: [   0    1    2 ... 9997 9998 9999]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:5984: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2862: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/vrishank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.09535+2.90096e-05\ttest-mlogloss:1.09575+3.47595e-05\n",
      "[50]\ttrain-mlogloss:0.975464+0.000969809\ttest-mlogloss:0.993868+0.000994213\n",
      "[100]\ttrain-mlogloss:0.906077+0.0011013\ttest-mlogloss:0.940519+0.00152211\n",
      "[150]\ttrain-mlogloss:0.861888+0.00125392\ttest-mlogloss:0.911004+0.00212437\n",
      "[200]\ttrain-mlogloss:0.830706+0.00145641\ttest-mlogloss:0.893812+0.00272792\n",
      "[250]\ttrain-mlogloss:0.807326+0.00147652\ttest-mlogloss:0.883595+0.00313695\n",
      "[300]\ttrain-mlogloss:0.78906+0.00151157\ttest-mlogloss:0.87758+0.00328482\n",
      "[350]\ttrain-mlogloss:0.774032+0.00109544\ttest-mlogloss:0.873907+0.00341195\n",
      "[400]\ttrain-mlogloss:0.761203+0.00107264\ttest-mlogloss:0.871644+0.00337422\n",
      "[450]\ttrain-mlogloss:0.749612+0.000958349\ttest-mlogloss:0.870303+0.00334533\n",
      "[500]\ttrain-mlogloss:0.738799+0.00114937\ttest-mlogloss:0.86958+0.00332968\n",
      "[550]\ttrain-mlogloss:0.728893+0.00124263\ttest-mlogloss:0.869098+0.00333223\n",
      "[600]\ttrain-mlogloss:0.719588+0.00196493\ttest-mlogloss:0.868818+0.0032915\n",
      "[650]\ttrain-mlogloss:0.710702+0.00229297\ttest-mlogloss:0.86874+0.0032799\n",
      "[700]\ttrain-mlogloss:0.702295+0.00266817\ttest-mlogloss:0.868849+0.00320938\n",
      "num_boost_rounds=659\n",
      "(10000, 3)\n",
      "[0]\ttrain-mlogloss:0.688209+4.07785e-05\ttest-mlogloss:0.688541+3.51884e-05\n",
      "[50]\ttrain-mlogloss:0.525627+0.00156868\ttest-mlogloss:0.540024+0.00216548\n",
      "[100]\ttrain-mlogloss:0.450464+0.00219465\ttest-mlogloss:0.47758+0.00397599\n",
      "[150]\ttrain-mlogloss:0.410642+0.00234374\ttest-mlogloss:0.450115+0.0052776\n",
      "[200]\ttrain-mlogloss:0.38633+0.0025997\ttest-mlogloss:0.438023+0.00619929\n",
      "[250]\ttrain-mlogloss:0.369221+0.00325354\ttest-mlogloss:0.43289+0.00673628\n",
      "[300]\ttrain-mlogloss:0.3558+0.00378328\ttest-mlogloss:0.430638+0.00707231\n",
      "[350]\ttrain-mlogloss:0.344782+0.00434122\ttest-mlogloss:0.429728+0.00720803\n",
      "[400]\ttrain-mlogloss:0.335183+0.0045531\ttest-mlogloss:0.429335+0.00720518\n",
      "[450]\ttrain-mlogloss:0.326785+0.00482585\ttest-mlogloss:0.429291+0.00721325\n",
      "num_boost_rounds=432\n",
      "(10000, 2)\n",
      "[0]\ttrain-mlogloss:0.688818+4.08982e-05\ttest-mlogloss:0.68916+3.48807e-05\n",
      "[50]\ttrain-mlogloss:0.545929+0.00171243\ttest-mlogloss:0.561548+0.00117262\n",
      "[100]\ttrain-mlogloss:0.478179+0.00266299\ttest-mlogloss:0.507249+0.00224215\n",
      "[150]\ttrain-mlogloss:0.439916+0.00296745\ttest-mlogloss:0.482364+0.00319484\n",
      "[200]\ttrain-mlogloss:0.413977+0.00264958\ttest-mlogloss:0.471046+0.00409356\n",
      "[250]\ttrain-mlogloss:0.394425+0.0025308\ttest-mlogloss:0.466013+0.00474813\n",
      "[300]\ttrain-mlogloss:0.379082+0.00282595\ttest-mlogloss:0.463626+0.00532802\n",
      "[350]\ttrain-mlogloss:0.365946+0.00342314\ttest-mlogloss:0.462795+0.00571554\n",
      "[400]\ttrain-mlogloss:0.354044+0.00349355\ttest-mlogloss:0.462589+0.00605936\n",
      "num_boost_rounds=393\n",
      "(10000, 2)\n",
      "[0]\ttrain-mlogloss:0.69169+1.18603e-05\ttest-mlogloss:0.691971+1.53261e-05\n",
      "[50]\ttrain-mlogloss:0.640515+0.00032621\ttest-mlogloss:0.654309+0.000419906\n",
      "[100]\ttrain-mlogloss:0.611271+0.000593512\ttest-mlogloss:0.637417+0.00104939\n",
      "[150]\ttrain-mlogloss:0.591668+0.000731624\ttest-mlogloss:0.62871+0.00142258\n",
      "[200]\ttrain-mlogloss:0.577161+0.000849593\ttest-mlogloss:0.623922+0.00163542\n",
      "[250]\ttrain-mlogloss:0.564936+0.000843611\ttest-mlogloss:0.621406+0.0018134\n",
      "[300]\ttrain-mlogloss:0.554503+0.000845659\ttest-mlogloss:0.619965+0.00207729\n",
      "[350]\ttrain-mlogloss:0.5455+0.00123326\ttest-mlogloss:0.619107+0.00238955\n",
      "[400]\ttrain-mlogloss:0.537673+0.00137659\ttest-mlogloss:0.618688+0.0024639\n",
      "[450]\ttrain-mlogloss:0.530787+0.00168648\ttest-mlogloss:0.618493+0.00250023\n",
      "[500]\ttrain-mlogloss:0.524333+0.00177332\ttest-mlogloss:0.618452+0.00264624\n",
      "[550]\ttrain-mlogloss:0.51805+0.00170679\ttest-mlogloss:0.618575+0.00281528\n",
      "num_boost_rounds=511\n",
      "TRAIN: [    0     1     2 ... 29997 29998 29999] TEST: [10000 10001 10002 ... 19997 19998 19999]\n",
      "[0]\ttrain-mlogloss:1.0954+3.26633e-05\ttest-mlogloss:1.09578+3.55059e-05\n",
      "[50]\ttrain-mlogloss:0.97659+0.00169715\ttest-mlogloss:0.994328+0.000907159\n",
      "[100]\ttrain-mlogloss:0.907994+0.00236334\ttest-mlogloss:0.940719+0.00187747\n",
      "[150]\ttrain-mlogloss:0.863737+0.00234322\ttest-mlogloss:0.910754+0.00272869\n",
      "[200]\ttrain-mlogloss:0.832717+0.00247456\ttest-mlogloss:0.89358+0.00332305\n",
      "[250]\ttrain-mlogloss:0.809358+0.0028438\ttest-mlogloss:0.883412+0.00352162\n",
      "[300]\ttrain-mlogloss:0.791191+0.00299219\ttest-mlogloss:0.877201+0.00361683\n",
      "[350]\ttrain-mlogloss:0.776513+0.00304621\ttest-mlogloss:0.873459+0.00356941\n",
      "[400]\ttrain-mlogloss:0.763761+0.00307282\ttest-mlogloss:0.87126+0.00367264\n",
      "[450]\ttrain-mlogloss:0.752346+0.00321875\ttest-mlogloss:0.869752+0.00366451\n",
      "[500]\ttrain-mlogloss:0.741805+0.00321823\ttest-mlogloss:0.868847+0.00349692\n",
      "[550]\ttrain-mlogloss:0.73194+0.00385517\ttest-mlogloss:0.868392+0.00347828\n",
      "[600]\ttrain-mlogloss:0.722505+0.00396929\ttest-mlogloss:0.868122+0.00347031\n",
      "[650]\ttrain-mlogloss:0.71372+0.00405046\ttest-mlogloss:0.868088+0.00343001\n",
      "[700]\ttrain-mlogloss:0.705559+0.00400463\ttest-mlogloss:0.86813+0.00340623\n",
      "num_boost_rounds=685\n",
      "(10000, 3)\n",
      "[0]\ttrain-mlogloss:0.688147+4.49691e-06\ttest-mlogloss:0.688506+4.37899e-05\n",
      "[50]\ttrain-mlogloss:0.523522+0.000312214\ttest-mlogloss:0.539181+0.000927398\n",
      "[100]\ttrain-mlogloss:0.447363+0.000821631\ttest-mlogloss:0.476641+0.0014208\n",
      "[150]\ttrain-mlogloss:0.407089+0.0012853\ttest-mlogloss:0.449393+0.001672\n",
      "[200]\ttrain-mlogloss:0.383177+0.00172172\ttest-mlogloss:0.437684+0.00187143\n",
      "[250]\ttrain-mlogloss:0.366469+0.00205036\ttest-mlogloss:0.432645+0.00191759\n",
      "[300]\ttrain-mlogloss:0.353823+0.00248118\ttest-mlogloss:0.430613+0.00194492\n",
      "[350]\ttrain-mlogloss:0.343518+0.00304467\ttest-mlogloss:0.429908+0.00209484\n",
      "[400]\ttrain-mlogloss:0.33427+0.00305876\ttest-mlogloss:0.429902+0.00214859\n",
      "num_boost_rounds=380\n",
      "(10000, 2)\n",
      "[0]\ttrain-mlogloss:0.688824+6.9062e-05\ttest-mlogloss:0.689156+7.92226e-05\n",
      "[50]\ttrain-mlogloss:0.545355+0.00225288\ttest-mlogloss:0.560911+0.00231025\n",
      "[100]\ttrain-mlogloss:0.476935+0.00357711\ttest-mlogloss:0.50666+0.00420802\n",
      "[150]\ttrain-mlogloss:0.438214+0.00379343\ttest-mlogloss:0.481905+0.00562063\n",
      "[200]\ttrain-mlogloss:0.412389+0.00401906\ttest-mlogloss:0.470619+0.0066235\n",
      "[250]\ttrain-mlogloss:0.39246+0.00463001\ttest-mlogloss:0.465324+0.00723244\n",
      "[300]\ttrain-mlogloss:0.376399+0.00534015\ttest-mlogloss:0.462675+0.00775298\n",
      "[350]\ttrain-mlogloss:0.363278+0.00600864\ttest-mlogloss:0.461395+0.00797379\n",
      "[400]\ttrain-mlogloss:0.352109+0.00566558\ttest-mlogloss:0.460942+0.00814181\n",
      "[450]\ttrain-mlogloss:0.341686+0.00576363\ttest-mlogloss:0.4609+0.00854727\n",
      "num_boost_rounds=440\n",
      "(10000, 2)\n",
      "[0]\ttrain-mlogloss:0.691725+3.18469e-05\ttest-mlogloss:0.692016+2.96011e-05\n",
      "[50]\ttrain-mlogloss:0.641382+0.00138952\ttest-mlogloss:0.654762+0.000770811\n",
      "[100]\ttrain-mlogloss:0.612773+0.00158692\ttest-mlogloss:0.637675+0.00143553\n",
      "[150]\ttrain-mlogloss:0.59345+0.00162541\ttest-mlogloss:0.628772+0.00176478\n",
      "[200]\ttrain-mlogloss:0.578835+0.00150696\ttest-mlogloss:0.624065+0.0018508\n",
      "[250]\ttrain-mlogloss:0.567102+0.00151584\ttest-mlogloss:0.621269+0.00190025\n",
      "[300]\ttrain-mlogloss:0.557114+0.00165202\ttest-mlogloss:0.619663+0.00179271\n",
      "[350]\ttrain-mlogloss:0.548172+0.0019006\ttest-mlogloss:0.618686+0.00182949\n",
      "[400]\ttrain-mlogloss:0.540173+0.00193746\ttest-mlogloss:0.618031+0.00182234\n",
      "[450]\ttrain-mlogloss:0.532825+0.00197289\ttest-mlogloss:0.617617+0.00187312\n",
      "[500]\ttrain-mlogloss:0.525929+0.00195952\ttest-mlogloss:0.617322+0.00191025\n",
      "[550]\ttrain-mlogloss:0.519374+0.00187178\ttest-mlogloss:0.61718+0.00194704\n",
      "[600]\ttrain-mlogloss:0.513297+0.00187098\ttest-mlogloss:0.617087+0.00205699\n",
      "[650]\ttrain-mlogloss:0.507404+0.00203261\ttest-mlogloss:0.617045+0.00208399\n",
      "[700]\ttrain-mlogloss:0.501822+0.00229364\ttest-mlogloss:0.616978+0.00210324\n",
      "num_boost_rounds=695\n",
      "TRAIN: [    0     1     2 ... 19997 19998 19999] TEST: [20000 20001 20002 ... 29997 29998 29999]\n",
      "[0]\ttrain-mlogloss:1.09535+4.82977e-05\ttest-mlogloss:1.09577+4.00777e-05\n",
      "[50]\ttrain-mlogloss:0.975344+0.00160458\ttest-mlogloss:0.994383+0.00138237\n",
      "[100]\ttrain-mlogloss:0.905676+0.00191394\ttest-mlogloss:0.940951+0.00262637\n",
      "[150]\ttrain-mlogloss:0.861089+0.00194299\ttest-mlogloss:0.911318+0.00355346\n",
      "[200]\ttrain-mlogloss:0.829949+0.00216956\ttest-mlogloss:0.894244+0.00439971\n",
      "[250]\ttrain-mlogloss:0.806801+0.00235036\ttest-mlogloss:0.884438+0.00510313\n",
      "[300]\ttrain-mlogloss:0.78877+0.00242287\ttest-mlogloss:0.878513+0.00556126\n",
      "[350]\ttrain-mlogloss:0.773686+0.00268585\ttest-mlogloss:0.875091+0.00592805\n",
      "[400]\ttrain-mlogloss:0.760594+0.00276221\ttest-mlogloss:0.872948+0.00611829\n",
      "[450]\ttrain-mlogloss:0.749093+0.00281857\ttest-mlogloss:0.871594+0.00625903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttrain-mlogloss:0.738404+0.00296201\ttest-mlogloss:0.870791+0.00650145\n",
      "[550]\ttrain-mlogloss:0.728569+0.00312808\ttest-mlogloss:0.870313+0.00662684\n",
      "[600]\ttrain-mlogloss:0.71925+0.00348445\ttest-mlogloss:0.870129+0.0066465\n",
      "[650]\ttrain-mlogloss:0.710582+0.003806\ttest-mlogloss:0.870025+0.006617\n",
      "[700]\ttrain-mlogloss:0.702037+0.00402837\ttest-mlogloss:0.870037+0.00667296\n",
      "num_boost_rounds=690\n",
      "(10000, 3)\n",
      "[0]\ttrain-mlogloss:0.688126+3.44125e-05\ttest-mlogloss:0.688425+4.04173e-05\n",
      "[50]\ttrain-mlogloss:0.523288+0.000971476\ttest-mlogloss:0.536618+0.0011595\n",
      "[100]\ttrain-mlogloss:0.447275+0.00132642\ttest-mlogloss:0.473109+0.00192058\n",
      "[150]\ttrain-mlogloss:0.406794+0.00119406\ttest-mlogloss:0.445082+0.00243507\n",
      "[200]\ttrain-mlogloss:0.382174+0.00125332\ttest-mlogloss:0.432795+0.00272994\n",
      "[250]\ttrain-mlogloss:0.365144+0.00150945\ttest-mlogloss:0.427228+0.00303136\n",
      "[300]\ttrain-mlogloss:0.351507+0.00187716\ttest-mlogloss:0.424687+0.00337781\n",
      "[350]\ttrain-mlogloss:0.339736+0.00223174\ttest-mlogloss:0.423448+0.00376458\n",
      "[400]\ttrain-mlogloss:0.32924+0.00251079\ttest-mlogloss:0.422948+0.00395986\n",
      "[450]\ttrain-mlogloss:0.319742+0.0028144\ttest-mlogloss:0.422766+0.00422338\n",
      "[500]\ttrain-mlogloss:0.310999+0.00301229\ttest-mlogloss:0.422791+0.00419775\n",
      "num_boost_rounds=481\n",
      "(10000, 2)\n",
      "[0]\ttrain-mlogloss:0.688719+9.4163e-06\ttest-mlogloss:0.689073+2.70678e-05\n",
      "[50]\ttrain-mlogloss:0.54264+0.00100179\ttest-mlogloss:0.55845+0.000985552\n",
      "[100]\ttrain-mlogloss:0.473389+0.00155642\ttest-mlogloss:0.502836+0.00174106\n",
      "[150]\ttrain-mlogloss:0.433773+0.00187347\ttest-mlogloss:0.477426+0.00273624\n",
      "[200]\ttrain-mlogloss:0.407444+0.0019765\ttest-mlogloss:0.465773+0.00371452\n",
      "[250]\ttrain-mlogloss:0.388914+0.00221595\ttest-mlogloss:0.459932+0.00451829\n",
      "[300]\ttrain-mlogloss:0.373939+0.00218286\ttest-mlogloss:0.456902+0.00503881\n",
      "[350]\ttrain-mlogloss:0.360865+0.00277435\ttest-mlogloss:0.45551+0.00549598\n",
      "[400]\ttrain-mlogloss:0.349416+0.00307111\ttest-mlogloss:0.45489+0.00591652\n",
      "[450]\ttrain-mlogloss:0.339187+0.00271838\ttest-mlogloss:0.454709+0.00610926\n",
      "[500]\ttrain-mlogloss:0.329775+0.00256482\ttest-mlogloss:0.454789+0.00629275\n",
      "num_boost_rounds=482\n",
      "(10000, 2)\n",
      "[0]\ttrain-mlogloss:0.691781+2.459e-05\ttest-mlogloss:0.692037+1.65395e-05\n",
      "[50]\ttrain-mlogloss:0.642275+0.00164428\ttest-mlogloss:0.655521+0.00116972\n",
      "[100]\ttrain-mlogloss:0.613774+0.00239587\ttest-mlogloss:0.63842+0.00233945\n",
      "[150]\ttrain-mlogloss:0.594296+0.00266148\ttest-mlogloss:0.62958+0.00306987\n",
      "[200]\ttrain-mlogloss:0.579715+0.00269742\ttest-mlogloss:0.624798+0.00344286\n",
      "[250]\ttrain-mlogloss:0.567905+0.00245031\ttest-mlogloss:0.62213+0.00358426\n",
      "[300]\ttrain-mlogloss:0.557461+0.00230202\ttest-mlogloss:0.620624+0.0038655\n",
      "[350]\ttrain-mlogloss:0.548504+0.00248365\ttest-mlogloss:0.61982+0.00400374\n",
      "[400]\ttrain-mlogloss:0.540634+0.00252289\ttest-mlogloss:0.619365+0.00408762\n",
      "[450]\ttrain-mlogloss:0.533543+0.00233239\ttest-mlogloss:0.619121+0.0041428\n",
      "[500]\ttrain-mlogloss:0.52699+0.00203732\ttest-mlogloss:0.619061+0.00426017\n",
      "[550]\ttrain-mlogloss:0.520909+0.00190732\ttest-mlogloss:0.619088+0.00429808\n",
      "num_boost_rounds=504\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    'n_trees': 700, \n",
    "    'eta': 0.01,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.93,\n",
    "    'objective': 'multi:softprob',\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'silent': 1,\n",
    "    'num_class' :2\n",
    "}\n",
    "xgb_params3 = {\n",
    "    'n_trees': 700, \n",
    "    'eta': 0.01,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.93,\n",
    "    'objective': 'multi:softprob',\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'silent': 1,\n",
    "    'num_class' :3\n",
    "}\n",
    "\n",
    "stack1=[]\n",
    "stack2=[]\n",
    "stack3=[]\n",
    "stack4=[]\n",
    "stack5=[]\n",
    "stack6=[]\n",
    "stack7=[]\n",
    "stack8=[]\n",
    "stack9=[]\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(X1):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X1.iloc[train_index], X1.iloc[test_index]\n",
    "    y_train, y_test = y1[train_index], y1[test_index]\n",
    "    \n",
    "    X_train[\"Target\"] = y_train\n",
    "    # High\n",
    "    X_train[\"Target\"][X_train[\"Target\"] == 0] = 3\n",
    "    # Low\n",
    "    X_train[\"Target\"][X_train[\"Target\"] == 1] = 0\n",
    "    # Medium\n",
    "    X_train[\"Target\"][X_train[\"Target\"] == 2] = 1\n",
    "    # High\n",
    "    X_train[\"Target\"][X_train[\"Target\"] == 3] = 2\n",
    "\n",
    "    X_MH = X_train[X_train[\"Target\"]!=0]\n",
    "    X_MH[\"Target\"][X_MH[\"Target\"]==1] = 0\n",
    "    X_MH[\"Target\"][X_MH[\"Target\"]==2] = 1\n",
    "    y_MH = X_MH[\"Target\"]\n",
    "    X_LH = X_train[X_train[\"Target\"]!=1]\n",
    "    X_LH[\"Target\"][X_LH[\"Target\"]==0] = 0\n",
    "    X_LH[\"Target\"][X_LH[\"Target\"]==2] = 1\n",
    "    y_LH = X_LH[\"Target\"]\n",
    "    X_LM = X_train[X_train[\"Target\"]!=2]\n",
    "    X_LM[\"Target\"][X_LM[\"Target\"]==0] = 0\n",
    "    X_LM[\"Target\"][X_LM[\"Target\"]==1] = 1\n",
    "    y_LM = X_LM[\"Target\"]\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train.drop([\"Target\"], axis=1), y_train)\n",
    "    dtrain_MH = xgb.DMatrix(X_MH.drop([\"Target\"], axis=1), y_MH)\n",
    "    dtrain_LH = xgb.DMatrix(X_LH.drop([\"Target\"], axis=1), y_LH)\n",
    "    dtrain_LM = xgb.DMatrix(X_LM.drop([\"Target\"], axis=1), y_LM)\n",
    "    \n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    cv_result3 = xgb.cv(xgb_params, \n",
    "                       dtrain_LH, \n",
    "                       num_boost_round=1200, # increase to have better results (~700)\n",
    "                       verbose_eval=50,\n",
    "                       early_stopping_rounds=50\n",
    "                      )\n",
    "    num_boost_rounds = len(cv_result3)\n",
    "    print('num_boost_rounds=' + str(num_boost_rounds))\n",
    "    # train model\n",
    "    model3 = xgb.train(dict(xgb_params, silent=1), dtrain_LH, num_boost_round=num_boost_rounds)\n",
    "    # make predictions and save results\n",
    "    y_preds3 = model3.predict(dtest)\n",
    "    print(y_preds3.shape)\n",
    "    stack6.append(y_preds3[:,0])\n",
    "    stack7.append(y_preds3[:,1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_result1 = xgb.cv(xgb_params3, \n",
    "                   dtrain, \n",
    "                   num_boost_round=1200, # increase to have better results (~700)\n",
    "                   verbose_eval=50,\n",
    "                   early_stopping_rounds=50\n",
    "                      )\n",
    "    num_boost_rounds = len(cv_result1)\n",
    "    print('num_boost_rounds=' + str(num_boost_rounds))\n",
    "    # train model\n",
    "    model1 = xgb.train(dict(xgb_params3, silent=1), dtrain, num_boost_round=num_boost_rounds)\n",
    "    # make predictions and save results\n",
    "    y_preds1 = model1.predict(dtest)\n",
    "    print(y_preds1.shape)\n",
    "    stack1.append(y_preds1[:,0])\n",
    "    stack2.append(y_preds1[:,1])\n",
    "    stack3.append(y_preds1[:,2])\n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_result2 = xgb.cv(xgb_params, \n",
    "                       dtrain_MH, \n",
    "                       num_boost_round=1200, # increase to have better results (~700)\n",
    "                       verbose_eval=50,\n",
    "                       early_stopping_rounds=50\n",
    "                      )\n",
    "    num_boost_rounds = len(cv_result2)\n",
    "    print('num_boost_rounds=' + str(num_boost_rounds))\n",
    "    # train model\n",
    "    model2 = xgb.train(dict(xgb_params, silent=1), dtrain_MH, num_boost_round=num_boost_rounds)\n",
    "    # make predictions and save results\n",
    "    y_preds2 = model2.predict(dtest)\n",
    "    print(y_preds2.shape)\n",
    "    stack4.append(y_preds2[:,0])\n",
    "    stack5.append(y_preds2[:,1])\n",
    "    \n",
    "    \n",
    "    cv_result3 = xgb.cv(xgb_params, \n",
    "                       dtrain_LH, \n",
    "                       num_boost_round=1200, # increase to have better results (~700)\n",
    "                       verbose_eval=50,\n",
    "                       early_stopping_rounds=50\n",
    "                      )\n",
    "    num_boost_rounds = len(cv_result3)\n",
    "    print('num_boost_rounds=' + str(num_boost_rounds))\n",
    "    # train model\n",
    "    model3 = xgb.train(dict(xgb_params, silent=1), dtrain_LH, num_boost_round=num_boost_rounds)\n",
    "    # make predictions and save results\n",
    "    y_preds3 = model3.predict(dtest)\n",
    "    print(y_preds3.shape)\n",
    "    stack6.append(y_preds3[:,0])\n",
    "    stack7.append(y_preds3[:,1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_result4 = xgb.cv(xgb_params, \n",
    "                       dtrain_LM, \n",
    "                       num_boost_round=1200, # increase to have better results (~700)\n",
    "                       verbose_eval=50,\n",
    "                       early_stopping_rounds=50\n",
    "                      )\n",
    "    num_boost_rounds = len(cv_result4)\n",
    "    print('num_boost_rounds=' + str(num_boost_rounds))\n",
    "    # train model\n",
    "    model4 = xgb.train(dict(xgb_params, silent=1), dtrain_LM, num_boost_round=num_boost_rounds)\n",
    "    # make 4predictions and save results\n",
    "    y_preds4 = model4.predict(dtest)\n",
    "    stack8.append(y_preds4[:,0])\n",
    "    stack9.append(y_preds4[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1[\"1\"]=np.array(stack1).ravel()\n",
    "X1[\"2\"]=np.array(stack2).ravel()\n",
    "X1[\"3\"]=np.array(stack3).ravel()\n",
    "X1[\"4\"]=np.array(stack4).ravel()\n",
    "X1[\"5\"]=np.array(stack5).ravel()\n",
    "X1[\"6\"]=np.array(stack6).ravel()\n",
    "X1[\"7\"]=np.array(stack7).ravel()\n",
    "X1[\"8\"]=np.array(stack8).ravel()\n",
    "X1[\"9\"]=np.array(stack9).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VAR1</th>\n",
       "      <th>VAR2</th>\n",
       "      <th>VAR3</th>\n",
       "      <th>VAR4</th>\n",
       "      <th>VAR5</th>\n",
       "      <th>VAR6</th>\n",
       "      <th>VAR7</th>\n",
       "      <th>VAR8</th>\n",
       "      <th>VAR9</th>\n",
       "      <th>VAR10</th>\n",
       "      <th>...</th>\n",
       "      <th>VAR20</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28892</th>\n",
       "      <td>28893</td>\n",
       "      <td>991.764706</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>142.857143</td>\n",
       "      <td>619.860682</td>\n",
       "      <td>5.181818</td>\n",
       "      <td>0.577611</td>\n",
       "      <td>197.098897</td>\n",
       "      <td>40.905027</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>120.003442</td>\n",
       "      <td>0.069733</td>\n",
       "      <td>0.521208</td>\n",
       "      <td>0.409059</td>\n",
       "      <td>0.886627</td>\n",
       "      <td>0.113373</td>\n",
       "      <td>0.881800</td>\n",
       "      <td>0.118200</td>\n",
       "      <td>0.575485</td>\n",
       "      <td>0.424515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19723</th>\n",
       "      <td>19724</td>\n",
       "      <td>883.529412</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>214.285714</td>\n",
       "      <td>667.759086</td>\n",
       "      <td>4.545455</td>\n",
       "      <td>2.243980</td>\n",
       "      <td>199.549597</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>104.509466</td>\n",
       "      <td>0.095210</td>\n",
       "      <td>0.476292</td>\n",
       "      <td>0.428498</td>\n",
       "      <td>0.819324</td>\n",
       "      <td>0.180676</td>\n",
       "      <td>0.829199</td>\n",
       "      <td>0.170801</td>\n",
       "      <td>0.522258</td>\n",
       "      <td>0.477742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17412</th>\n",
       "      <td>17413</td>\n",
       "      <td>977.647059</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>650.171829</td>\n",
       "      <td>27.272727</td>\n",
       "      <td>2.621698</td>\n",
       "      <td>201.604241</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>173.485370</td>\n",
       "      <td>0.125235</td>\n",
       "      <td>0.339465</td>\n",
       "      <td>0.535299</td>\n",
       "      <td>0.824647</td>\n",
       "      <td>0.175353</td>\n",
       "      <td>0.751186</td>\n",
       "      <td>0.248814</td>\n",
       "      <td>0.364622</td>\n",
       "      <td>0.635378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20476</th>\n",
       "      <td>20477</td>\n",
       "      <td>937.647059</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>22.727364</td>\n",
       "      <td>1.564687</td>\n",
       "      <td>198.627684</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.899686</td>\n",
       "      <td>...</td>\n",
       "      <td>137.266781</td>\n",
       "      <td>0.084380</td>\n",
       "      <td>0.165119</td>\n",
       "      <td>0.750501</td>\n",
       "      <td>0.889884</td>\n",
       "      <td>0.110115</td>\n",
       "      <td>0.647848</td>\n",
       "      <td>0.352152</td>\n",
       "      <td>0.198330</td>\n",
       "      <td>0.801670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7365</th>\n",
       "      <td>7366</td>\n",
       "      <td>850.588235</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>71.428571</td>\n",
       "      <td>653.892008</td>\n",
       "      <td>13.636364</td>\n",
       "      <td>2.697167</td>\n",
       "      <td>201.382237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>152.339071</td>\n",
       "      <td>0.248287</td>\n",
       "      <td>0.287712</td>\n",
       "      <td>0.464001</td>\n",
       "      <td>0.560431</td>\n",
       "      <td>0.439569</td>\n",
       "      <td>0.543162</td>\n",
       "      <td>0.456838</td>\n",
       "      <td>0.362119</td>\n",
       "      <td>0.637881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6355</th>\n",
       "      <td>6356</td>\n",
       "      <td>928.235294</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>669.691829</td>\n",
       "      <td>4.545455</td>\n",
       "      <td>0.667120</td>\n",
       "      <td>197.812847</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>106.641997</td>\n",
       "      <td>0.088491</td>\n",
       "      <td>0.430769</td>\n",
       "      <td>0.480740</td>\n",
       "      <td>0.837878</td>\n",
       "      <td>0.162122</td>\n",
       "      <td>0.794726</td>\n",
       "      <td>0.205274</td>\n",
       "      <td>0.503147</td>\n",
       "      <td>0.496853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30546</th>\n",
       "      <td>30547</td>\n",
       "      <td>972.941177</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>627.957543</td>\n",
       "      <td>15.909091</td>\n",
       "      <td>1.141893</td>\n",
       "      <td>198.285085</td>\n",
       "      <td>0.688738</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>155.746988</td>\n",
       "      <td>0.046883</td>\n",
       "      <td>0.315020</td>\n",
       "      <td>0.638098</td>\n",
       "      <td>0.935171</td>\n",
       "      <td>0.064829</td>\n",
       "      <td>0.875387</td>\n",
       "      <td>0.124613</td>\n",
       "      <td>0.325730</td>\n",
       "      <td>0.674270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058</th>\n",
       "      <td>2059</td>\n",
       "      <td>896.470588</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>892.857143</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>8.181818</td>\n",
       "      <td>4.467330</td>\n",
       "      <td>206.799863</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>80.356282</td>\n",
       "      <td>0.197111</td>\n",
       "      <td>0.298047</td>\n",
       "      <td>0.504842</td>\n",
       "      <td>0.592775</td>\n",
       "      <td>0.407225</td>\n",
       "      <td>0.825751</td>\n",
       "      <td>0.174249</td>\n",
       "      <td>0.234667</td>\n",
       "      <td>0.765333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24729</th>\n",
       "      <td>24730</td>\n",
       "      <td>918.823529</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>535.714286</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>1.093286</td>\n",
       "      <td>199.009949</td>\n",
       "      <td>29.098096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>102.235800</td>\n",
       "      <td>0.066087</td>\n",
       "      <td>0.481951</td>\n",
       "      <td>0.451963</td>\n",
       "      <td>0.869674</td>\n",
       "      <td>0.130326</td>\n",
       "      <td>0.883589</td>\n",
       "      <td>0.116411</td>\n",
       "      <td>0.491479</td>\n",
       "      <td>0.508521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5892</th>\n",
       "      <td>5893</td>\n",
       "      <td>891.764706</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>682.238900</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>1.882524</td>\n",
       "      <td>199.577731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>119.423408</td>\n",
       "      <td>0.111957</td>\n",
       "      <td>0.411169</td>\n",
       "      <td>0.476874</td>\n",
       "      <td>0.794063</td>\n",
       "      <td>0.205937</td>\n",
       "      <td>0.783771</td>\n",
       "      <td>0.216229</td>\n",
       "      <td>0.466189</td>\n",
       "      <td>0.533811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25282</th>\n",
       "      <td>25283</td>\n",
       "      <td>948.235294</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>71.428571</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>6.818182</td>\n",
       "      <td>0.658974</td>\n",
       "      <td>197.145931</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>97.025818</td>\n",
       "      <td>0.106482</td>\n",
       "      <td>0.119194</td>\n",
       "      <td>0.774324</td>\n",
       "      <td>0.884810</td>\n",
       "      <td>0.115190</td>\n",
       "      <td>0.431921</td>\n",
       "      <td>0.568079</td>\n",
       "      <td>0.116136</td>\n",
       "      <td>0.883864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17692</th>\n",
       "      <td>17693</td>\n",
       "      <td>865.882353</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>616.907297</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.892935</td>\n",
       "      <td>197.499255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>110.759036</td>\n",
       "      <td>0.070592</td>\n",
       "      <td>0.523612</td>\n",
       "      <td>0.405796</td>\n",
       "      <td>0.874541</td>\n",
       "      <td>0.125459</td>\n",
       "      <td>0.876037</td>\n",
       "      <td>0.123963</td>\n",
       "      <td>0.528731</td>\n",
       "      <td>0.471269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13838</th>\n",
       "      <td>13839</td>\n",
       "      <td>996.470588</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>71.428571</td>\n",
       "      <td>611.608010</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>0.563587</td>\n",
       "      <td>196.914789</td>\n",
       "      <td>21.196863</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>103.067126</td>\n",
       "      <td>0.055462</td>\n",
       "      <td>0.277435</td>\n",
       "      <td>0.667103</td>\n",
       "      <td>0.922982</td>\n",
       "      <td>0.077018</td>\n",
       "      <td>0.847677</td>\n",
       "      <td>0.152323</td>\n",
       "      <td>0.302186</td>\n",
       "      <td>0.697814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3253</th>\n",
       "      <td>3254</td>\n",
       "      <td>957.647059</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>0.559710</td>\n",
       "      <td>196.911295</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>47.899686</td>\n",
       "      <td>...</td>\n",
       "      <td>107.672978</td>\n",
       "      <td>0.066856</td>\n",
       "      <td>0.243305</td>\n",
       "      <td>0.689840</td>\n",
       "      <td>0.904805</td>\n",
       "      <td>0.095195</td>\n",
       "      <td>0.800503</td>\n",
       "      <td>0.199497</td>\n",
       "      <td>0.281526</td>\n",
       "      <td>0.718474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23644</th>\n",
       "      <td>23645</td>\n",
       "      <td>915.294118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>3.636364</td>\n",
       "      <td>0.559710</td>\n",
       "      <td>196.911295</td>\n",
       "      <td>13.522358</td>\n",
       "      <td>47.899686</td>\n",
       "      <td>...</td>\n",
       "      <td>185.385542</td>\n",
       "      <td>0.021808</td>\n",
       "      <td>0.497339</td>\n",
       "      <td>0.480854</td>\n",
       "      <td>0.960179</td>\n",
       "      <td>0.039821</td>\n",
       "      <td>0.951092</td>\n",
       "      <td>0.048908</td>\n",
       "      <td>0.543471</td>\n",
       "      <td>0.456529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13792</th>\n",
       "      <td>13793</td>\n",
       "      <td>981.176471</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>620.412143</td>\n",
       "      <td>5.454545</td>\n",
       "      <td>0.734734</td>\n",
       "      <td>197.498579</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>104.519794</td>\n",
       "      <td>0.122985</td>\n",
       "      <td>0.313135</td>\n",
       "      <td>0.563881</td>\n",
       "      <td>0.870740</td>\n",
       "      <td>0.129260</td>\n",
       "      <td>0.792018</td>\n",
       "      <td>0.207982</td>\n",
       "      <td>0.348420</td>\n",
       "      <td>0.651580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26239</th>\n",
       "      <td>26240</td>\n",
       "      <td>968.235294</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>178.571429</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>10.090909</td>\n",
       "      <td>0.559710</td>\n",
       "      <td>196.922914</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>89.543890</td>\n",
       "      <td>0.163088</td>\n",
       "      <td>0.247497</td>\n",
       "      <td>0.589415</td>\n",
       "      <td>0.852251</td>\n",
       "      <td>0.147749</td>\n",
       "      <td>0.737562</td>\n",
       "      <td>0.262438</td>\n",
       "      <td>0.218793</td>\n",
       "      <td>0.781207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25836</th>\n",
       "      <td>25837</td>\n",
       "      <td>875.294118</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>714.285714</td>\n",
       "      <td>675.637114</td>\n",
       "      <td>36.363636</td>\n",
       "      <td>5.369634</td>\n",
       "      <td>207.654990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>63.127367</td>\n",
       "      <td>0.365699</td>\n",
       "      <td>0.026024</td>\n",
       "      <td>0.608277</td>\n",
       "      <td>0.633032</td>\n",
       "      <td>0.366968</td>\n",
       "      <td>0.185752</td>\n",
       "      <td>0.814248</td>\n",
       "      <td>0.048874</td>\n",
       "      <td>0.951126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9477</th>\n",
       "      <td>9478</td>\n",
       "      <td>929.411765</td>\n",
       "      <td>0.105522</td>\n",
       "      <td>71.428571</td>\n",
       "      <td>616.600929</td>\n",
       "      <td>13.636364</td>\n",
       "      <td>0.813763</td>\n",
       "      <td>197.425561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>134.910499</td>\n",
       "      <td>0.072176</td>\n",
       "      <td>0.313225</td>\n",
       "      <td>0.614599</td>\n",
       "      <td>0.879495</td>\n",
       "      <td>0.120505</td>\n",
       "      <td>0.816559</td>\n",
       "      <td>0.183441</td>\n",
       "      <td>0.355501</td>\n",
       "      <td>0.644499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11144</th>\n",
       "      <td>11145</td>\n",
       "      <td>877.647059</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>142.857143</td>\n",
       "      <td>614.550891</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.678910</td>\n",
       "      <td>197.129644</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>105.172117</td>\n",
       "      <td>0.079441</td>\n",
       "      <td>0.321916</td>\n",
       "      <td>0.598643</td>\n",
       "      <td>0.874415</td>\n",
       "      <td>0.125585</td>\n",
       "      <td>0.831621</td>\n",
       "      <td>0.168379</td>\n",
       "      <td>0.355263</td>\n",
       "      <td>0.644737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541</th>\n",
       "      <td>2542</td>\n",
       "      <td>983.529412</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>617.756373</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.861936</td>\n",
       "      <td>197.559303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>121.089501</td>\n",
       "      <td>0.080271</td>\n",
       "      <td>0.201095</td>\n",
       "      <td>0.718634</td>\n",
       "      <td>0.887650</td>\n",
       "      <td>0.112350</td>\n",
       "      <td>0.718716</td>\n",
       "      <td>0.281284</td>\n",
       "      <td>0.202643</td>\n",
       "      <td>0.797357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4679</th>\n",
       "      <td>4680</td>\n",
       "      <td>858.823529</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>142.857143</td>\n",
       "      <td>645.157902</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>2.695309</td>\n",
       "      <td>200.994628</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>145.669535</td>\n",
       "      <td>0.125539</td>\n",
       "      <td>0.334529</td>\n",
       "      <td>0.539932</td>\n",
       "      <td>0.815623</td>\n",
       "      <td>0.184378</td>\n",
       "      <td>0.775067</td>\n",
       "      <td>0.224933</td>\n",
       "      <td>0.423589</td>\n",
       "      <td>0.576411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31618</th>\n",
       "      <td>31619</td>\n",
       "      <td>982.352941</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>633.773277</td>\n",
       "      <td>90.909091</td>\n",
       "      <td>0.756431</td>\n",
       "      <td>197.851301</td>\n",
       "      <td>350.721463</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>127.795181</td>\n",
       "      <td>0.087978</td>\n",
       "      <td>0.349151</td>\n",
       "      <td>0.562871</td>\n",
       "      <td>0.847639</td>\n",
       "      <td>0.152361</td>\n",
       "      <td>0.817545</td>\n",
       "      <td>0.182455</td>\n",
       "      <td>0.379531</td>\n",
       "      <td>0.620469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26015</th>\n",
       "      <td>26016</td>\n",
       "      <td>940.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>616.023207</td>\n",
       "      <td>3.863636</td>\n",
       "      <td>0.681255</td>\n",
       "      <td>197.383144</td>\n",
       "      <td>3.388076</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>701.814114</td>\n",
       "      <td>0.038839</td>\n",
       "      <td>0.251000</td>\n",
       "      <td>0.710160</td>\n",
       "      <td>0.943487</td>\n",
       "      <td>0.056513</td>\n",
       "      <td>0.841693</td>\n",
       "      <td>0.158307</td>\n",
       "      <td>0.284693</td>\n",
       "      <td>0.715307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32595</th>\n",
       "      <td>32596</td>\n",
       "      <td>978.823529</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>637.349902</td>\n",
       "      <td>18.181818</td>\n",
       "      <td>2.221234</td>\n",
       "      <td>200.693543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>111.166953</td>\n",
       "      <td>0.102277</td>\n",
       "      <td>0.270599</td>\n",
       "      <td>0.627124</td>\n",
       "      <td>0.820607</td>\n",
       "      <td>0.179393</td>\n",
       "      <td>0.750720</td>\n",
       "      <td>0.249280</td>\n",
       "      <td>0.288688</td>\n",
       "      <td>0.711312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5079</th>\n",
       "      <td>5080</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>0.072222</td>\n",
       "      <td>964.285714</td>\n",
       "      <td>802.396321</td>\n",
       "      <td>5.909091</td>\n",
       "      <td>0.625348</td>\n",
       "      <td>198.431385</td>\n",
       "      <td>3.884309</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>198.734940</td>\n",
       "      <td>0.025775</td>\n",
       "      <td>0.840482</td>\n",
       "      <td>0.133743</td>\n",
       "      <td>0.886473</td>\n",
       "      <td>0.113527</td>\n",
       "      <td>0.956512</td>\n",
       "      <td>0.043488</td>\n",
       "      <td>0.834945</td>\n",
       "      <td>0.165055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10683</th>\n",
       "      <td>10684</td>\n",
       "      <td>927.058823</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>612.458837</td>\n",
       "      <td>36.363636</td>\n",
       "      <td>0.580565</td>\n",
       "      <td>197.004013</td>\n",
       "      <td>16.213141</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>110.359725</td>\n",
       "      <td>0.036498</td>\n",
       "      <td>0.254931</td>\n",
       "      <td>0.708571</td>\n",
       "      <td>0.948616</td>\n",
       "      <td>0.051384</td>\n",
       "      <td>0.887084</td>\n",
       "      <td>0.112915</td>\n",
       "      <td>0.267136</td>\n",
       "      <td>0.732865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28357</th>\n",
       "      <td>28358</td>\n",
       "      <td>915.294118</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>648.801052</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>2.215964</td>\n",
       "      <td>200.251304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>158.628227</td>\n",
       "      <td>0.134177</td>\n",
       "      <td>0.276393</td>\n",
       "      <td>0.589430</td>\n",
       "      <td>0.805098</td>\n",
       "      <td>0.194902</td>\n",
       "      <td>0.667528</td>\n",
       "      <td>0.332472</td>\n",
       "      <td>0.310164</td>\n",
       "      <td>0.689836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23130</th>\n",
       "      <td>23131</td>\n",
       "      <td>903.529412</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>687.343862</td>\n",
       "      <td>45.454545</td>\n",
       "      <td>3.168786</td>\n",
       "      <td>201.676341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>210.366609</td>\n",
       "      <td>0.083427</td>\n",
       "      <td>0.374050</td>\n",
       "      <td>0.542523</td>\n",
       "      <td>0.882577</td>\n",
       "      <td>0.117423</td>\n",
       "      <td>0.831608</td>\n",
       "      <td>0.168392</td>\n",
       "      <td>0.416000</td>\n",
       "      <td>0.584000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10236</th>\n",
       "      <td>10237</td>\n",
       "      <td>940.000000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>631.492151</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>1.431556</td>\n",
       "      <td>200.200644</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.182235</td>\n",
       "      <td>...</td>\n",
       "      <td>131.438899</td>\n",
       "      <td>0.269998</td>\n",
       "      <td>0.163455</td>\n",
       "      <td>0.566547</td>\n",
       "      <td>0.705735</td>\n",
       "      <td>0.294265</td>\n",
       "      <td>0.355186</td>\n",
       "      <td>0.644814</td>\n",
       "      <td>0.213662</td>\n",
       "      <td>0.786338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27952</th>\n",
       "      <td>27953</td>\n",
       "      <td>922.352941</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>428.571429</td>\n",
       "      <td>710.584042</td>\n",
       "      <td>22.727273</td>\n",
       "      <td>6.432720</td>\n",
       "      <td>207.289049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.259819</td>\n",
       "      <td>0.185440</td>\n",
       "      <td>0.554741</td>\n",
       "      <td>0.540686</td>\n",
       "      <td>0.459314</td>\n",
       "      <td>0.510536</td>\n",
       "      <td>0.489464</td>\n",
       "      <td>0.291108</td>\n",
       "      <td>0.708892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14555</th>\n",
       "      <td>14556</td>\n",
       "      <td>801.176471</td>\n",
       "      <td>0.017778</td>\n",
       "      <td>428.571429</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>1.454545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.899686</td>\n",
       "      <td>...</td>\n",
       "      <td>103.049914</td>\n",
       "      <td>0.046031</td>\n",
       "      <td>0.767144</td>\n",
       "      <td>0.186825</td>\n",
       "      <td>0.840259</td>\n",
       "      <td>0.159741</td>\n",
       "      <td>0.939523</td>\n",
       "      <td>0.060477</td>\n",
       "      <td>0.777708</td>\n",
       "      <td>0.222292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12358</th>\n",
       "      <td>12359</td>\n",
       "      <td>855.294118</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>357.142857</td>\n",
       "      <td>716.634367</td>\n",
       "      <td>3.909091</td>\n",
       "      <td>1.051171</td>\n",
       "      <td>199.333934</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>81.148021</td>\n",
       "      <td>0.230331</td>\n",
       "      <td>0.119416</td>\n",
       "      <td>0.650254</td>\n",
       "      <td>0.750911</td>\n",
       "      <td>0.249089</td>\n",
       "      <td>0.295160</td>\n",
       "      <td>0.704840</td>\n",
       "      <td>0.163379</td>\n",
       "      <td>0.836621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20767</th>\n",
       "      <td>20768</td>\n",
       "      <td>983.529412</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>5.454545</td>\n",
       "      <td>2.038966</td>\n",
       "      <td>199.929645</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>138.321859</td>\n",
       "      <td>0.104850</td>\n",
       "      <td>0.442773</td>\n",
       "      <td>0.452377</td>\n",
       "      <td>0.828900</td>\n",
       "      <td>0.171100</td>\n",
       "      <td>0.777624</td>\n",
       "      <td>0.222376</td>\n",
       "      <td>0.465415</td>\n",
       "      <td>0.534585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27833</th>\n",
       "      <td>27834</td>\n",
       "      <td>851.764706</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>620.167049</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>1.410425</td>\n",
       "      <td>198.172064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>110.790017</td>\n",
       "      <td>0.063175</td>\n",
       "      <td>0.427650</td>\n",
       "      <td>0.509175</td>\n",
       "      <td>0.898374</td>\n",
       "      <td>0.101626</td>\n",
       "      <td>0.898444</td>\n",
       "      <td>0.101556</td>\n",
       "      <td>0.467373</td>\n",
       "      <td>0.532627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5020</th>\n",
       "      <td>5021</td>\n",
       "      <td>884.705882</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.646687</td>\n",
       "      <td>197.137527</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>109.851979</td>\n",
       "      <td>0.018820</td>\n",
       "      <td>0.517747</td>\n",
       "      <td>0.463433</td>\n",
       "      <td>0.964846</td>\n",
       "      <td>0.035154</td>\n",
       "      <td>0.961728</td>\n",
       "      <td>0.038272</td>\n",
       "      <td>0.518924</td>\n",
       "      <td>0.481076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32301</th>\n",
       "      <td>32302</td>\n",
       "      <td>995.294118</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.559710</td>\n",
       "      <td>196.911295</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>47.899686</td>\n",
       "      <td>...</td>\n",
       "      <td>146.913941</td>\n",
       "      <td>0.106365</td>\n",
       "      <td>0.173266</td>\n",
       "      <td>0.720369</td>\n",
       "      <td>0.856851</td>\n",
       "      <td>0.143149</td>\n",
       "      <td>0.574584</td>\n",
       "      <td>0.425416</td>\n",
       "      <td>0.208277</td>\n",
       "      <td>0.791723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18784</th>\n",
       "      <td>18785</td>\n",
       "      <td>968.235294</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>71.428571</td>\n",
       "      <td>618.353352</td>\n",
       "      <td>5.454545</td>\n",
       "      <td>1.131062</td>\n",
       "      <td>197.366049</td>\n",
       "      <td>0.752906</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>109.075731</td>\n",
       "      <td>0.105818</td>\n",
       "      <td>0.237671</td>\n",
       "      <td>0.656511</td>\n",
       "      <td>0.837788</td>\n",
       "      <td>0.162212</td>\n",
       "      <td>0.759310</td>\n",
       "      <td>0.240690</td>\n",
       "      <td>0.302742</td>\n",
       "      <td>0.697258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>9992</td>\n",
       "      <td>964.705882</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>619.092136</td>\n",
       "      <td>31.818182</td>\n",
       "      <td>1.627679</td>\n",
       "      <td>199.171795</td>\n",
       "      <td>7.511946</td>\n",
       "      <td>48.148689</td>\n",
       "      <td>...</td>\n",
       "      <td>126.817556</td>\n",
       "      <td>0.089368</td>\n",
       "      <td>0.319852</td>\n",
       "      <td>0.590779</td>\n",
       "      <td>0.870938</td>\n",
       "      <td>0.129062</td>\n",
       "      <td>0.813562</td>\n",
       "      <td>0.186438</td>\n",
       "      <td>0.359327</td>\n",
       "      <td>0.640673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>872</td>\n",
       "      <td>860.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>178.571429</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>7.290909</td>\n",
       "      <td>0.563956</td>\n",
       "      <td>196.941411</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>94.452668</td>\n",
       "      <td>0.189307</td>\n",
       "      <td>0.189675</td>\n",
       "      <td>0.621017</td>\n",
       "      <td>0.750134</td>\n",
       "      <td>0.249866</td>\n",
       "      <td>0.532806</td>\n",
       "      <td>0.467194</td>\n",
       "      <td>0.212918</td>\n",
       "      <td>0.787082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28766</th>\n",
       "      <td>28767</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>615.240656</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>0.807085</td>\n",
       "      <td>197.423220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>107.975904</td>\n",
       "      <td>0.078614</td>\n",
       "      <td>0.214401</td>\n",
       "      <td>0.706985</td>\n",
       "      <td>0.900546</td>\n",
       "      <td>0.099454</td>\n",
       "      <td>0.761463</td>\n",
       "      <td>0.238537</td>\n",
       "      <td>0.218078</td>\n",
       "      <td>0.781922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22767</th>\n",
       "      <td>22768</td>\n",
       "      <td>937.647059</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>107.142857</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>2.012017</td>\n",
       "      <td>199.787984</td>\n",
       "      <td>20.580850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>115.936317</td>\n",
       "      <td>0.091984</td>\n",
       "      <td>0.430362</td>\n",
       "      <td>0.477654</td>\n",
       "      <td>0.847833</td>\n",
       "      <td>0.152167</td>\n",
       "      <td>0.817482</td>\n",
       "      <td>0.182518</td>\n",
       "      <td>0.441221</td>\n",
       "      <td>0.558779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20926</th>\n",
       "      <td>20927</td>\n",
       "      <td>991.764706</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>621.315491</td>\n",
       "      <td>22.727273</td>\n",
       "      <td>0.847289</td>\n",
       "      <td>197.156325</td>\n",
       "      <td>99.139720</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>107.179002</td>\n",
       "      <td>0.065232</td>\n",
       "      <td>0.155859</td>\n",
       "      <td>0.778909</td>\n",
       "      <td>0.912042</td>\n",
       "      <td>0.087958</td>\n",
       "      <td>0.763739</td>\n",
       "      <td>0.236261</td>\n",
       "      <td>0.187172</td>\n",
       "      <td>0.812828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11679</th>\n",
       "      <td>11680</td>\n",
       "      <td>935.294118</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>4.545455</td>\n",
       "      <td>0.559710</td>\n",
       "      <td>196.911295</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>47.899686</td>\n",
       "      <td>...</td>\n",
       "      <td>107.299484</td>\n",
       "      <td>0.063845</td>\n",
       "      <td>0.339640</td>\n",
       "      <td>0.596515</td>\n",
       "      <td>0.897313</td>\n",
       "      <td>0.102687</td>\n",
       "      <td>0.880408</td>\n",
       "      <td>0.119592</td>\n",
       "      <td>0.409970</td>\n",
       "      <td>0.590030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13999</th>\n",
       "      <td>14000</td>\n",
       "      <td>977.647059</td>\n",
       "      <td>0.072222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>623.013643</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.003237</td>\n",
       "      <td>197.880132</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>124.500861</td>\n",
       "      <td>0.094973</td>\n",
       "      <td>0.368204</td>\n",
       "      <td>0.536823</td>\n",
       "      <td>0.873587</td>\n",
       "      <td>0.126413</td>\n",
       "      <td>0.781082</td>\n",
       "      <td>0.218918</td>\n",
       "      <td>0.401548</td>\n",
       "      <td>0.598452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25664</th>\n",
       "      <td>25665</td>\n",
       "      <td>891.764706</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>668.602911</td>\n",
       "      <td>6.590909</td>\n",
       "      <td>1.500062</td>\n",
       "      <td>201.286854</td>\n",
       "      <td>307.921338</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>107.397590</td>\n",
       "      <td>0.220643</td>\n",
       "      <td>0.294144</td>\n",
       "      <td>0.485213</td>\n",
       "      <td>0.705339</td>\n",
       "      <td>0.294661</td>\n",
       "      <td>0.638811</td>\n",
       "      <td>0.361189</td>\n",
       "      <td>0.369592</td>\n",
       "      <td>0.630408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3975</th>\n",
       "      <td>3976</td>\n",
       "      <td>923.529412</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>5.909091</td>\n",
       "      <td>0.559710</td>\n",
       "      <td>196.911295</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>47.899686</td>\n",
       "      <td>...</td>\n",
       "      <td>104.521515</td>\n",
       "      <td>0.051565</td>\n",
       "      <td>0.285618</td>\n",
       "      <td>0.662818</td>\n",
       "      <td>0.941591</td>\n",
       "      <td>0.058409</td>\n",
       "      <td>0.868699</td>\n",
       "      <td>0.131301</td>\n",
       "      <td>0.295208</td>\n",
       "      <td>0.704792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27415</th>\n",
       "      <td>27416</td>\n",
       "      <td>978.823529</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>614.781980</td>\n",
       "      <td>18.181818</td>\n",
       "      <td>0.687280</td>\n",
       "      <td>197.185552</td>\n",
       "      <td>53.939708</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>111.196213</td>\n",
       "      <td>0.071589</td>\n",
       "      <td>0.276608</td>\n",
       "      <td>0.651803</td>\n",
       "      <td>0.892943</td>\n",
       "      <td>0.107057</td>\n",
       "      <td>0.783783</td>\n",
       "      <td>0.216217</td>\n",
       "      <td>0.299850</td>\n",
       "      <td>0.700150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21525</th>\n",
       "      <td>21526</td>\n",
       "      <td>922.352941</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>214.285714</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>3.638486</td>\n",
       "      <td>204.026278</td>\n",
       "      <td>4.675716</td>\n",
       "      <td>47.899686</td>\n",
       "      <td>...</td>\n",
       "      <td>48.709122</td>\n",
       "      <td>0.690926</td>\n",
       "      <td>0.015876</td>\n",
       "      <td>0.293198</td>\n",
       "      <td>0.312055</td>\n",
       "      <td>0.687945</td>\n",
       "      <td>0.037363</td>\n",
       "      <td>0.962637</td>\n",
       "      <td>0.065052</td>\n",
       "      <td>0.934947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21319</th>\n",
       "      <td>21320</td>\n",
       "      <td>988.235294</td>\n",
       "      <td>1.944444</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>624.650522</td>\n",
       "      <td>2.727273</td>\n",
       "      <td>0.980302</td>\n",
       "      <td>197.711651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>111.029260</td>\n",
       "      <td>0.095951</td>\n",
       "      <td>0.166764</td>\n",
       "      <td>0.737285</td>\n",
       "      <td>0.894274</td>\n",
       "      <td>0.105726</td>\n",
       "      <td>0.484748</td>\n",
       "      <td>0.515252</td>\n",
       "      <td>0.205335</td>\n",
       "      <td>0.794665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28046</th>\n",
       "      <td>28047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>71.428571</td>\n",
       "      <td>626.446712</td>\n",
       "      <td>2.727273</td>\n",
       "      <td>1.310817</td>\n",
       "      <td>198.429542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>155.537005</td>\n",
       "      <td>0.099221</td>\n",
       "      <td>0.485968</td>\n",
       "      <td>0.414811</td>\n",
       "      <td>0.829717</td>\n",
       "      <td>0.170283</td>\n",
       "      <td>0.844090</td>\n",
       "      <td>0.155910</td>\n",
       "      <td>0.486697</td>\n",
       "      <td>0.513303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5151</th>\n",
       "      <td>5152</td>\n",
       "      <td>857.647059</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>321.428571</td>\n",
       "      <td>712.637581</td>\n",
       "      <td>5.454545</td>\n",
       "      <td>1.769597</td>\n",
       "      <td>201.289724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>136.180723</td>\n",
       "      <td>0.090069</td>\n",
       "      <td>0.511292</td>\n",
       "      <td>0.398639</td>\n",
       "      <td>0.827399</td>\n",
       "      <td>0.172601</td>\n",
       "      <td>0.862247</td>\n",
       "      <td>0.137753</td>\n",
       "      <td>0.552646</td>\n",
       "      <td>0.447354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13155</th>\n",
       "      <td>13156</td>\n",
       "      <td>831.764706</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>428.571429</td>\n",
       "      <td>618.432132</td>\n",
       "      <td>5.454545</td>\n",
       "      <td>0.611134</td>\n",
       "      <td>197.405369</td>\n",
       "      <td>6.301308</td>\n",
       "      <td>55.832344</td>\n",
       "      <td>...</td>\n",
       "      <td>97.824441</td>\n",
       "      <td>0.056345</td>\n",
       "      <td>0.607406</td>\n",
       "      <td>0.336249</td>\n",
       "      <td>0.868420</td>\n",
       "      <td>0.131580</td>\n",
       "      <td>0.903308</td>\n",
       "      <td>0.096692</td>\n",
       "      <td>0.642963</td>\n",
       "      <td>0.357037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7690</th>\n",
       "      <td>7691</td>\n",
       "      <td>912.941177</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>71.428571</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>22.727364</td>\n",
       "      <td>2.588637</td>\n",
       "      <td>201.191956</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>136.569707</td>\n",
       "      <td>0.109767</td>\n",
       "      <td>0.435291</td>\n",
       "      <td>0.454941</td>\n",
       "      <td>0.837354</td>\n",
       "      <td>0.162646</td>\n",
       "      <td>0.795157</td>\n",
       "      <td>0.204843</td>\n",
       "      <td>0.507775</td>\n",
       "      <td>0.492225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31574</th>\n",
       "      <td>31575</td>\n",
       "      <td>945.882353</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>627.799982</td>\n",
       "      <td>5.181818</td>\n",
       "      <td>0.871471</td>\n",
       "      <td>197.591584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>100.271945</td>\n",
       "      <td>0.061485</td>\n",
       "      <td>0.574737</td>\n",
       "      <td>0.363778</td>\n",
       "      <td>0.881944</td>\n",
       "      <td>0.118056</td>\n",
       "      <td>0.846681</td>\n",
       "      <td>0.153319</td>\n",
       "      <td>0.550055</td>\n",
       "      <td>0.449945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18836</th>\n",
       "      <td>18837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>464.285714</td>\n",
       "      <td>690.540590</td>\n",
       "      <td>18.181818</td>\n",
       "      <td>1.813065</td>\n",
       "      <td>205.326111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>427.738382</td>\n",
       "      <td>0.069567</td>\n",
       "      <td>0.320986</td>\n",
       "      <td>0.609447</td>\n",
       "      <td>0.887941</td>\n",
       "      <td>0.112059</td>\n",
       "      <td>0.849045</td>\n",
       "      <td>0.150955</td>\n",
       "      <td>0.299514</td>\n",
       "      <td>0.700486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18854</th>\n",
       "      <td>18855</td>\n",
       "      <td>832.941176</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>607.142857</td>\n",
       "      <td>681.428339</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>3.199287</td>\n",
       "      <td>200.745121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.367446</td>\n",
       "      <td>...</td>\n",
       "      <td>408.006885</td>\n",
       "      <td>0.100732</td>\n",
       "      <td>0.280876</td>\n",
       "      <td>0.618392</td>\n",
       "      <td>0.849445</td>\n",
       "      <td>0.150555</td>\n",
       "      <td>0.799768</td>\n",
       "      <td>0.200232</td>\n",
       "      <td>0.316174</td>\n",
       "      <td>0.683826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2134</th>\n",
       "      <td>2135</td>\n",
       "      <td>882.352941</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>695.498495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.270372</td>\n",
       "      <td>210.704300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>219.098107</td>\n",
       "      <td>0.123394</td>\n",
       "      <td>0.443298</td>\n",
       "      <td>0.433308</td>\n",
       "      <td>0.785705</td>\n",
       "      <td>0.214295</td>\n",
       "      <td>0.773923</td>\n",
       "      <td>0.226077</td>\n",
       "      <td>0.490693</td>\n",
       "      <td>0.509307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25208</th>\n",
       "      <td>25209</td>\n",
       "      <td>987.058823</td>\n",
       "      <td>0.038889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>620.504929</td>\n",
       "      <td>6.363636</td>\n",
       "      <td>1.502665</td>\n",
       "      <td>198.783460</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>...</td>\n",
       "      <td>100.022375</td>\n",
       "      <td>0.125031</td>\n",
       "      <td>0.180993</td>\n",
       "      <td>0.693976</td>\n",
       "      <td>0.868415</td>\n",
       "      <td>0.131585</td>\n",
       "      <td>0.429974</td>\n",
       "      <td>0.570027</td>\n",
       "      <td>0.197243</td>\n",
       "      <td>0.802757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12413</th>\n",
       "      <td>12414</td>\n",
       "      <td>954.117647</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>321.428571</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>22.727364</td>\n",
       "      <td>4.738561</td>\n",
       "      <td>205.562538</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>111.855422</td>\n",
       "      <td>0.072731</td>\n",
       "      <td>0.475448</td>\n",
       "      <td>0.451821</td>\n",
       "      <td>0.856694</td>\n",
       "      <td>0.143306</td>\n",
       "      <td>0.870516</td>\n",
       "      <td>0.129484</td>\n",
       "      <td>0.504085</td>\n",
       "      <td>0.495915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        VAR1         VAR2      VAR3        VAR4        VAR5       VAR6  \\\n",
       "28892  28893   991.764706  0.083333  142.857143  619.860682   5.181818   \n",
       "19723  19724   883.529412  0.166667  214.285714  667.759086   4.545455   \n",
       "17412  17413   977.647059  0.138889   35.714286  650.171829  27.272727   \n",
       "20476  20477   937.647059  0.833333         NaN  611.574748  22.727364   \n",
       "7365    7366   850.588235  4.444444   71.428571  653.892008  13.636364   \n",
       "6355    6356   928.235294  0.166667   35.714286  669.691829   4.545455   \n",
       "30546  30547   972.941177  0.555556         NaN  627.957543  15.909091   \n",
       "2058    2059   896.470588  1.666667  892.857143  611.574748   8.181818   \n",
       "24729  24730   918.823529  0.138889  535.714286  611.574748  11.363636   \n",
       "5892    5893   891.764706  0.027778   35.714286  682.238900   9.090909   \n",
       "25282  25283   948.235294  0.833333   71.428571  611.574748   6.818182   \n",
       "17692  17693   865.882353  0.055556  250.000000  616.907297  10.000000   \n",
       "13838  13839   996.470588  0.166667   71.428571  611.608010  11.363636   \n",
       "3253    3254   957.647059  0.083333         NaN  611.574748   9.090909   \n",
       "23644  23645   915.294118       NaN         NaN  611.574748   3.636364   \n",
       "13792  13793   981.176471  0.222222         NaN  620.412143   5.454545   \n",
       "26239  26240   968.235294  0.388889  178.571429  611.574748  10.090909   \n",
       "25836  25837   875.294118  0.388889  714.285714  675.637114  36.363636   \n",
       "9477    9478   929.411765  0.105522   71.428571  616.600929  13.636364   \n",
       "11144  11145   877.647059  0.222222  142.857143  614.550891        NaN   \n",
       "2541    2542   983.529412  0.388889         NaN  617.756373        NaN   \n",
       "4679    4680   858.823529  0.555556  142.857143  645.157902   9.090909   \n",
       "31618  31619   982.352941  0.111111         NaN  633.773277  90.909091   \n",
       "26015  26016   940.000000       NaN         NaN  616.023207   3.863636   \n",
       "32595  32596   978.823529  0.007778         NaN  637.349902  18.181818   \n",
       "5079    5080   800.000000  0.072222  964.285714  802.396321   5.909091   \n",
       "10683  10684   927.058823  0.888889         NaN  612.458837  36.363636   \n",
       "28357  28358   915.294118  0.444444   35.714286  648.801052   9.090909   \n",
       "23130  23131   903.529412  0.222222         NaN  687.343862  45.454545   \n",
       "10236  10237   940.000000  1.666667         NaN  631.492151   9.090909   \n",
       "...      ...          ...       ...         ...         ...        ...   \n",
       "27952  27953   922.352941  0.138889  428.571429  710.584042  22.727273   \n",
       "14555  14556   801.176471  0.017778  428.571429  611.574748   1.454545   \n",
       "12358  12359   855.294118  0.027778  357.142857  716.634367   3.909091   \n",
       "20767  20768   983.529412  0.055556         NaN  611.574748   5.454545   \n",
       "27833  27834   851.764706  0.111111         NaN  620.167049   9.090909   \n",
       "5020    5021   884.705882       NaN         NaN  611.574748        NaN   \n",
       "32301  32302   995.294118  2.333333         NaN  611.574748  15.000000   \n",
       "18784  18785   968.235294  0.555556   71.428571  618.353352   5.454545   \n",
       "9991    9992   964.705882  0.016667   35.714286  619.092136  31.818182   \n",
       "871      872   860.000000  0.833333  178.571429  611.574748   7.290909   \n",
       "28766  28767  1000.000000  0.388889         NaN  615.240656  11.363636   \n",
       "22767  22768   937.647059  0.027778  107.142857  611.574748  11.363636   \n",
       "20926  20927   991.764706  0.388889         NaN  621.315491  22.727273   \n",
       "11679  11680   935.294118  0.138889         NaN  611.574748   4.545455   \n",
       "13999  14000   977.647059  0.072222         NaN  623.013643        NaN   \n",
       "25664  25665   891.764706  3.333333         NaN  668.602911   6.590909   \n",
       "3975    3976   923.529412  0.361111         NaN  611.574748   5.909091   \n",
       "27415  27416   978.823529  0.166667         NaN  614.781980  18.181818   \n",
       "21525  21526   922.352941  1.666667  214.285714  611.574748  11.363636   \n",
       "21319  21320   988.235294  1.944444   35.714286  624.650522   2.727273   \n",
       "28046  28047          NaN  0.166667   71.428571  626.446712   2.727273   \n",
       "5151    5152   857.647059  0.533333  321.428571  712.637581   5.454545   \n",
       "13155  13156   831.764706  0.055556  428.571429  618.432132   5.454545   \n",
       "7690    7691   912.941177  0.166667   71.428571  611.574748  22.727364   \n",
       "31574  31575   945.882353  0.138889         NaN  627.799982   5.181818   \n",
       "18836  18837          NaN       NaN  464.285714  690.540590  18.181818   \n",
       "18854  18855   832.941176  0.388889  607.142857  681.428339   9.090909   \n",
       "2134    2135   882.352941  0.138889   35.714286  695.498495        NaN   \n",
       "25208  25209   987.058823  0.038889         NaN  620.504929   6.363636   \n",
       "12413  12414   954.117647  0.027778  321.428571  611.574748  22.727364   \n",
       "\n",
       "           VAR7        VAR8        VAR9      VAR10    ...           VAR20  \\\n",
       "28892  0.577611  197.098897   40.905027  58.632548    ...      120.003442   \n",
       "19723  2.243980  199.549597         NaN  58.632548    ...      104.509466   \n",
       "17412  2.621698  201.604241         NaN  58.632548    ...      173.485370   \n",
       "20476  1.564687  198.627684         NaN  47.899686    ...      137.266781   \n",
       "7365   2.697167  201.382237         NaN  58.632548    ...      152.339071   \n",
       "6355   0.667120  197.812847         NaN  58.632548    ...      106.641997   \n",
       "30546  1.141893  198.285085    0.688738  58.632548    ...      155.746988   \n",
       "2058   4.467330  206.799863   22.086661        NaN    ...       80.356282   \n",
       "24729  1.093286  199.009949   29.098096        NaN    ...      102.235800   \n",
       "5892   1.882524  199.577731         NaN  58.632548    ...      119.423408   \n",
       "25282  0.658974  197.145931   22.086661        NaN    ...       97.025818   \n",
       "17692  0.892935  197.499255         NaN  58.632548    ...      110.759036   \n",
       "13838  0.563587  196.914789   21.196863  58.632548    ...      103.067126   \n",
       "3253   0.559710  196.911295   22.086661  47.899686    ...      107.672978   \n",
       "23644  0.559710  196.911295   13.522358  47.899686    ...      185.385542   \n",
       "13792  0.734734  197.498579         NaN  58.632548    ...      104.519794   \n",
       "26239  0.559710  196.922914   22.086661        NaN    ...       89.543890   \n",
       "25836  5.369634  207.654990         NaN  58.632548    ...       63.127367   \n",
       "9477   0.813763  197.425561         NaN  58.632548    ...      134.910499   \n",
       "11144  0.678910  197.129644         NaN  58.632548    ...      105.172117   \n",
       "2541   0.861936  197.559303         NaN  58.632548    ...      121.089501   \n",
       "4679   2.695309  200.994628         NaN  58.632548    ...      145.669535   \n",
       "31618  0.756431  197.851301  350.721463  58.632548    ...      127.795181   \n",
       "26015  0.681255  197.383144    3.388076  58.632548    ...      701.814114   \n",
       "32595  2.221234  200.693543         NaN  58.632548    ...      111.166953   \n",
       "5079   0.625348  198.431385    3.884309  58.632548    ...      198.734940   \n",
       "10683  0.580565  197.004013   16.213141  58.632548    ...      110.359725   \n",
       "28357  2.215964  200.251304         NaN  58.632548    ...      158.628227   \n",
       "23130  3.168786  201.676341         NaN  58.632548    ...      210.366609   \n",
       "10236  1.431556  200.200644         NaN  56.182235    ...      131.438899   \n",
       "...         ...         ...         ...        ...    ...             ...   \n",
       "27952  6.432720  207.289049         NaN  58.632548    ...     1000.000000   \n",
       "14555       NaN         NaN         NaN  47.899686    ...      103.049914   \n",
       "12358  1.051171  199.333934         NaN  58.632548    ...       81.148021   \n",
       "20767  2.038966  199.929645   22.086661        NaN    ...      138.321859   \n",
       "27833  1.410425  198.172064         NaN  58.632548    ...      110.790017   \n",
       "5020   0.646687  197.137527   22.086661        NaN    ...      109.851979   \n",
       "32301  0.559710  196.911295   22.086661  47.899686    ...      146.913941   \n",
       "18784  1.131062  197.366049    0.752906  58.632548    ...      109.075731   \n",
       "9991   1.627679  199.171795    7.511946  48.148689    ...      126.817556   \n",
       "871    0.563956  196.941411   22.086661        NaN    ...       94.452668   \n",
       "28766  0.807085  197.423220         NaN  58.632548    ...      107.975904   \n",
       "22767  2.012017  199.787984   20.580850        NaN    ...      115.936317   \n",
       "20926  0.847289  197.156325   99.139720  58.632548    ...      107.179002   \n",
       "11679  0.559710  196.911295   22.086661  47.899686    ...      107.299484   \n",
       "13999  1.003237  197.880132         NaN  58.632548    ...      124.500861   \n",
       "25664  1.500062  201.286854  307.921338  58.632548    ...      107.397590   \n",
       "3975   0.559710  196.911295   22.086661  47.899686    ...      104.521515   \n",
       "27415  0.687280  197.185552   53.939708  58.632548    ...      111.196213   \n",
       "21525  3.638486  204.026278    4.675716  47.899686    ...       48.709122   \n",
       "21319  0.980302  197.711651         NaN  58.632548    ...      111.029260   \n",
       "28046  1.310817  198.429542         NaN  58.632548    ...      155.537005   \n",
       "5151   1.769597  201.289724         NaN  58.632548    ...      136.180723   \n",
       "13155  0.611134  197.405369    6.301308  55.832344    ...       97.824441   \n",
       "7690   2.588637  201.191956   22.086661        NaN    ...      136.569707   \n",
       "31574  0.871471  197.591584         NaN  58.632548    ...      100.271945   \n",
       "18836  1.813065  205.326111         NaN  58.632548    ...      427.738382   \n",
       "18854  3.199287  200.745121         NaN  58.367446    ...      408.006885   \n",
       "2134   8.270372  210.704300         NaN  58.632548    ...      219.098107   \n",
       "25208  1.502665  198.783460         NaN  58.632548    ...      100.022375   \n",
       "12413  4.738561  205.562538   22.086661        NaN    ...      111.855422   \n",
       "\n",
       "              1         2         3         4         5         6         7  \\\n",
       "28892  0.069733  0.521208  0.409059  0.886627  0.113373  0.881800  0.118200   \n",
       "19723  0.095210  0.476292  0.428498  0.819324  0.180676  0.829199  0.170801   \n",
       "17412  0.125235  0.339465  0.535299  0.824647  0.175353  0.751186  0.248814   \n",
       "20476  0.084380  0.165119  0.750501  0.889884  0.110115  0.647848  0.352152   \n",
       "7365   0.248287  0.287712  0.464001  0.560431  0.439569  0.543162  0.456838   \n",
       "6355   0.088491  0.430769  0.480740  0.837878  0.162122  0.794726  0.205274   \n",
       "30546  0.046883  0.315020  0.638098  0.935171  0.064829  0.875387  0.124613   \n",
       "2058   0.197111  0.298047  0.504842  0.592775  0.407225  0.825751  0.174249   \n",
       "24729  0.066087  0.481951  0.451963  0.869674  0.130326  0.883589  0.116411   \n",
       "5892   0.111957  0.411169  0.476874  0.794063  0.205937  0.783771  0.216229   \n",
       "25282  0.106482  0.119194  0.774324  0.884810  0.115190  0.431921  0.568079   \n",
       "17692  0.070592  0.523612  0.405796  0.874541  0.125459  0.876037  0.123963   \n",
       "13838  0.055462  0.277435  0.667103  0.922982  0.077018  0.847677  0.152323   \n",
       "3253   0.066856  0.243305  0.689840  0.904805  0.095195  0.800503  0.199497   \n",
       "23644  0.021808  0.497339  0.480854  0.960179  0.039821  0.951092  0.048908   \n",
       "13792  0.122985  0.313135  0.563881  0.870740  0.129260  0.792018  0.207982   \n",
       "26239  0.163088  0.247497  0.589415  0.852251  0.147749  0.737562  0.262438   \n",
       "25836  0.365699  0.026024  0.608277  0.633032  0.366968  0.185752  0.814248   \n",
       "9477   0.072176  0.313225  0.614599  0.879495  0.120505  0.816559  0.183441   \n",
       "11144  0.079441  0.321916  0.598643  0.874415  0.125585  0.831621  0.168379   \n",
       "2541   0.080271  0.201095  0.718634  0.887650  0.112350  0.718716  0.281284   \n",
       "4679   0.125539  0.334529  0.539932  0.815623  0.184378  0.775067  0.224933   \n",
       "31618  0.087978  0.349151  0.562871  0.847639  0.152361  0.817545  0.182455   \n",
       "26015  0.038839  0.251000  0.710160  0.943487  0.056513  0.841693  0.158307   \n",
       "32595  0.102277  0.270599  0.627124  0.820607  0.179393  0.750720  0.249280   \n",
       "5079   0.025775  0.840482  0.133743  0.886473  0.113527  0.956512  0.043488   \n",
       "10683  0.036498  0.254931  0.708571  0.948616  0.051384  0.887084  0.112915   \n",
       "28357  0.134177  0.276393  0.589430  0.805098  0.194902  0.667528  0.332472   \n",
       "23130  0.083427  0.374050  0.542523  0.882577  0.117423  0.831608  0.168392   \n",
       "10236  0.269998  0.163455  0.566547  0.705735  0.294265  0.355186  0.644814   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "27952  0.259819  0.185440  0.554741  0.540686  0.459314  0.510536  0.489464   \n",
       "14555  0.046031  0.767144  0.186825  0.840259  0.159741  0.939523  0.060477   \n",
       "12358  0.230331  0.119416  0.650254  0.750911  0.249089  0.295160  0.704840   \n",
       "20767  0.104850  0.442773  0.452377  0.828900  0.171100  0.777624  0.222376   \n",
       "27833  0.063175  0.427650  0.509175  0.898374  0.101626  0.898444  0.101556   \n",
       "5020   0.018820  0.517747  0.463433  0.964846  0.035154  0.961728  0.038272   \n",
       "32301  0.106365  0.173266  0.720369  0.856851  0.143149  0.574584  0.425416   \n",
       "18784  0.105818  0.237671  0.656511  0.837788  0.162212  0.759310  0.240690   \n",
       "9991   0.089368  0.319852  0.590779  0.870938  0.129062  0.813562  0.186438   \n",
       "871    0.189307  0.189675  0.621017  0.750134  0.249866  0.532806  0.467194   \n",
       "28766  0.078614  0.214401  0.706985  0.900546  0.099454  0.761463  0.238537   \n",
       "22767  0.091984  0.430362  0.477654  0.847833  0.152167  0.817482  0.182518   \n",
       "20926  0.065232  0.155859  0.778909  0.912042  0.087958  0.763739  0.236261   \n",
       "11679  0.063845  0.339640  0.596515  0.897313  0.102687  0.880408  0.119592   \n",
       "13999  0.094973  0.368204  0.536823  0.873587  0.126413  0.781082  0.218918   \n",
       "25664  0.220643  0.294144  0.485213  0.705339  0.294661  0.638811  0.361189   \n",
       "3975   0.051565  0.285618  0.662818  0.941591  0.058409  0.868699  0.131301   \n",
       "27415  0.071589  0.276608  0.651803  0.892943  0.107057  0.783783  0.216217   \n",
       "21525  0.690926  0.015876  0.293198  0.312055  0.687945  0.037363  0.962637   \n",
       "21319  0.095951  0.166764  0.737285  0.894274  0.105726  0.484748  0.515252   \n",
       "28046  0.099221  0.485968  0.414811  0.829717  0.170283  0.844090  0.155910   \n",
       "5151   0.090069  0.511292  0.398639  0.827399  0.172601  0.862247  0.137753   \n",
       "13155  0.056345  0.607406  0.336249  0.868420  0.131580  0.903308  0.096692   \n",
       "7690   0.109767  0.435291  0.454941  0.837354  0.162646  0.795157  0.204843   \n",
       "31574  0.061485  0.574737  0.363778  0.881944  0.118056  0.846681  0.153319   \n",
       "18836  0.069567  0.320986  0.609447  0.887941  0.112059  0.849045  0.150955   \n",
       "18854  0.100732  0.280876  0.618392  0.849445  0.150555  0.799768  0.200232   \n",
       "2134   0.123394  0.443298  0.433308  0.785705  0.214295  0.773923  0.226077   \n",
       "25208  0.125031  0.180993  0.693976  0.868415  0.131585  0.429974  0.570027   \n",
       "12413  0.072731  0.475448  0.451821  0.856694  0.143306  0.870516  0.129484   \n",
       "\n",
       "              8         9  \n",
       "28892  0.575485  0.424515  \n",
       "19723  0.522258  0.477742  \n",
       "17412  0.364622  0.635378  \n",
       "20476  0.198330  0.801670  \n",
       "7365   0.362119  0.637881  \n",
       "6355   0.503147  0.496853  \n",
       "30546  0.325730  0.674270  \n",
       "2058   0.234667  0.765333  \n",
       "24729  0.491479  0.508521  \n",
       "5892   0.466189  0.533811  \n",
       "25282  0.116136  0.883864  \n",
       "17692  0.528731  0.471269  \n",
       "13838  0.302186  0.697814  \n",
       "3253   0.281526  0.718474  \n",
       "23644  0.543471  0.456529  \n",
       "13792  0.348420  0.651580  \n",
       "26239  0.218793  0.781207  \n",
       "25836  0.048874  0.951126  \n",
       "9477   0.355501  0.644499  \n",
       "11144  0.355263  0.644737  \n",
       "2541   0.202643  0.797357  \n",
       "4679   0.423589  0.576411  \n",
       "31618  0.379531  0.620469  \n",
       "26015  0.284693  0.715307  \n",
       "32595  0.288688  0.711312  \n",
       "5079   0.834945  0.165055  \n",
       "10683  0.267136  0.732865  \n",
       "28357  0.310164  0.689836  \n",
       "23130  0.416000  0.584000  \n",
       "10236  0.213662  0.786338  \n",
       "...         ...       ...  \n",
       "27952  0.291108  0.708892  \n",
       "14555  0.777708  0.222292  \n",
       "12358  0.163379  0.836621  \n",
       "20767  0.465415  0.534585  \n",
       "27833  0.467373  0.532627  \n",
       "5020   0.518924  0.481076  \n",
       "32301  0.208277  0.791723  \n",
       "18784  0.302742  0.697258  \n",
       "9991   0.359327  0.640673  \n",
       "871    0.212918  0.787082  \n",
       "28766  0.218078  0.781922  \n",
       "22767  0.441221  0.558779  \n",
       "20926  0.187172  0.812828  \n",
       "11679  0.409970  0.590030  \n",
       "13999  0.401548  0.598452  \n",
       "25664  0.369592  0.630408  \n",
       "3975   0.295208  0.704792  \n",
       "27415  0.299850  0.700150  \n",
       "21525  0.065052  0.934947  \n",
       "21319  0.205335  0.794665  \n",
       "28046  0.486697  0.513303  \n",
       "5151   0.552646  0.447354  \n",
       "13155  0.642963  0.357037  \n",
       "7690   0.507775  0.492225  \n",
       "31574  0.550055  0.449945  \n",
       "18836  0.299514  0.700486  \n",
       "18854  0.316174  0.683826  \n",
       "2134   0.490693  0.509307  \n",
       "25208  0.197243  0.802757  \n",
       "12413  0.504085  0.495915  \n",
       "\n",
       "[30000 rows x 29 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.09513+1.29701e-05\ttest-mlogloss:1.09544+2.12027e-05\n",
      "[50]\ttrain-mlogloss:0.970053+0.000487679\ttest-mlogloss:0.984294+0.000385843\n",
      "[100]\ttrain-mlogloss:0.902113+0.000785687\ttest-mlogloss:0.928754+0.000753237\n",
      "[150]\ttrain-mlogloss:0.861594+0.00106352\ttest-mlogloss:0.899743+0.00089951\n",
      "[200]\ttrain-mlogloss:0.835001+0.00133308\ttest-mlogloss:0.884189+0.000990048\n",
      "[250]\ttrain-mlogloss:0.816102+0.00154439\ttest-mlogloss:0.875672+0.00112451\n",
      "[300]\ttrain-mlogloss:0.802049+0.00157738\ttest-mlogloss:0.870993+0.00114317\n",
      "[350]\ttrain-mlogloss:0.79105+0.00120495\ttest-mlogloss:0.868414+0.0011282\n",
      "[400]\ttrain-mlogloss:0.781973+0.000911362\ttest-mlogloss:0.867119+0.00109064\n",
      "[450]\ttrain-mlogloss:0.773899+0.000562617\ttest-mlogloss:0.866554+0.00103412\n",
      "[500]\ttrain-mlogloss:0.766558+0.000493576\ttest-mlogloss:0.866389+0.000937905\n",
      "[550]\ttrain-mlogloss:0.75994+0.000493372\ttest-mlogloss:0.866387+0.000874062\n",
      "num_boost_rounds=509\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X1, y1)\n",
    "cv_result1 = xgb.cv(xgb_params3, \n",
    "                   dtrain, \n",
    "                   num_boost_round=1200, # increase to have better results (~700)\n",
    "                   verbose_eval=50,\n",
    "                   early_stopping_rounds=50\n",
    "                      )\n",
    "num_boost_rounds = len(cv_result1)\n",
    "print('num_boost_rounds=' + str(num_boost_rounds))\n",
    "# train model\n",
    "model1 = xgb.train(dict(xgb_params3, silent=1), dtrain, num_boost_round=num_boost_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.09549+4.03072e-05\ttest-mlogloss:1.09575+1.84451e-05\n",
      "[50]\ttrain-mlogloss:0.98153+0.00135223\ttest-mlogloss:0.993748+0.0012607\n",
      "[100]\ttrain-mlogloss:0.917237+0.0018322\ttest-mlogloss:0.939922+0.00238902\n",
      "[150]\ttrain-mlogloss:0.877228+0.00194761\ttest-mlogloss:0.909662+0.00337444\n",
      "[200]\ttrain-mlogloss:0.850482+0.00215229\ttest-mlogloss:0.892066+0.00420626\n",
      "[250]\ttrain-mlogloss:0.831015+0.00234337\ttest-mlogloss:0.881289+0.0047568\n",
      "[300]\ttrain-mlogloss:0.816068+0.00247658\ttest-mlogloss:0.87457+0.00522031\n",
      "[350]\ttrain-mlogloss:0.804266+0.00223424\ttest-mlogloss:0.870389+0.00549063\n",
      "[400]\ttrain-mlogloss:0.79421+0.00229057\ttest-mlogloss:0.86764+0.00568867\n",
      "[450]\ttrain-mlogloss:0.785585+0.00222746\ttest-mlogloss:0.865839+0.00586117\n",
      "[500]\ttrain-mlogloss:0.777664+0.00221564\ttest-mlogloss:0.864716+0.00596999\n",
      "[550]\ttrain-mlogloss:0.770397+0.00247084\ttest-mlogloss:0.863956+0.0060063\n",
      "[600]\ttrain-mlogloss:0.763576+0.0026513\ttest-mlogloss:0.863403+0.00610004\n",
      "[650]\ttrain-mlogloss:0.757198+0.00279597\ttest-mlogloss:0.863026+0.00618939\n",
      "[700]\ttrain-mlogloss:0.750741+0.00294575\ttest-mlogloss:0.862801+0.00626694\n",
      "[750]\ttrain-mlogloss:0.74467+0.00310278\ttest-mlogloss:0.862694+0.00627188\n",
      "[800]\ttrain-mlogloss:0.739115+0.00325481\ttest-mlogloss:0.862625+0.00632318\n",
      "[850]\ttrain-mlogloss:0.733552+0.00308917\ttest-mlogloss:0.862604+0.00630289\n",
      "[900]\ttrain-mlogloss:0.728102+0.0032182\ttest-mlogloss:0.862612+0.00627482\n",
      "num_boost_rounds=861\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# prepare dict of params for xgboost to run with\n",
    "xgb_params = {\n",
    "    'n_trees': 700, \n",
    "    'eta': 0.01,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.90,\n",
    "    'objective': 'multi:softmax',\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'silent': 1,\n",
    "    'num_class' :3\n",
    "}\n",
    "\n",
    "# form DMatrices for Xgboost training\n",
    "dtrain = xgb.DMatrix(X, y)\n",
    "dtest = xgb.DMatrix(leaderboard)\n",
    "\n",
    "# xgboost, cross-validation\n",
    "cv_result = xgb.cv(xgb_params, \n",
    "                   dtrain, \n",
    "                   num_boost_round=1200, # increase to have better results (~700)\n",
    "                   verbose_eval=50,\n",
    "                   early_stopping_rounds=50\n",
    "                  )\n",
    "\n",
    "num_boost_rounds = len(cv_result)\n",
    "print('num_boost_rounds=' + str(num_boost_rounds))\n",
    "\n",
    "# train model\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n",
    "\n",
    "\n",
    "# check f2-score (to get higher score - increase num_boost_round in previous cell)\n",
    "\n",
    "# make predictions and save results\n",
    "y_preds = model.predict(dtest)\n",
    "\n",
    "d = {'col1': leaderboard[\"VAR1\"], 'col2': [int(i) for i in y_preds]}\n",
    "df = pd.DataFrame(data=d)\n",
    "df[\"col2\"][df[\"col2\"] == 0] = \"High\"\n",
    "df[\"col2\"][df[\"col2\"] == 1] = \"Low\"\n",
    "df[\"col2\"][df[\"col2\"] == 2] = \"Medium\"\n",
    "\n",
    "import shutil \n",
    "\n",
    "filename = \"Quant404_IITGuwahati_25\"\n",
    "df.to_csv(filename+'.csv', index=False, header=False)\n",
    "shutil.copyfile(\"Quant404_IITGuwahati_23.ipynb\", filename+\".ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719362ece2a14f2abcc93ef5ca289e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped pipeline #14 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #17 due to time out. Continuing to the next pipeline.\n"
     ]
    }
   ],
   "source": [
    "tpot.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X.drop([\"Target\"], axis=1), y)\n",
    "dtrain_MH = xgb.DMatrix(X_MH.drop([\"Target\"], axis=1), y_MH)\n",
    "dtrain_LH = xgb.DMatrix(X_LH.drop([\"Target\"], axis=1), y_LH)\n",
    "dtrain_LM = xgb.DMatrix(X_LM.drop([\"Target\"], axis=1), y_LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.408662+0.00149306\ttest-merror:0.436471+0.00255931\n",
      "[50]\ttrain-merror:0.396221+0.000600838\ttest-merror:0.428059+0.00412513\n",
      "[100]\ttrain-merror:0.390368+0.000141827\ttest-merror:0.426383+0.00473133\n",
      "[150]\ttrain-merror:0.384941+0.000487107\ttest-merror:0.424147+0.00544673\n",
      "[200]\ttrain-merror:0.379809+0.000348081\ttest-merror:0.423353+0.00553797\n",
      "[250]\ttrain-merror:0.374044+0.000988193\ttest-merror:0.422118+0.00505074\n",
      "[300]\ttrain-merror:0.368838+0.00117597\ttest-merror:0.421794+0.0051521\n"
     ]
    }
   ],
   "source": [
    "cv_result = xgb.cv(xgb_params3, \n",
    "                   dtrain, \n",
    "                   num_boost_round=1200, # increase to have better results (~700)\n",
    "                   verbose_eval=50,\n",
    "                   early_stopping_rounds=50\n",
    "                  )\n",
    "cv_result = xgb.cv(xgb_params, \n",
    "                   dtrain_MH, \n",
    "                   num_boost_round=1200, # increase to have better results (~700)\n",
    "                   verbose_eval=50,\n",
    "                   early_stopping_rounds=50\n",
    "                  )\n",
    "cv_result = xgb.cv(xgb_params, \n",
    "                   dtrain_LH, \n",
    "                   num_boost_round=1200, # increase to have better results (~700)\n",
    "                   verbose_eval=50,\n",
    "                   early_stopping_rounds=50\n",
    "                  )\n",
    "cv_result = xgb.cv(xgb_params, \n",
    "                   dtrain_LM, \n",
    "                   num_boost_round=1200, # increase to have better results (~700)\n",
    "                   verbose_eval=50,\n",
    "                   early_stopping_rounds=50\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.348891+0.000938942\ttest-merror:0.368228+0.00217272\n",
      "[50]\ttrain-merror:0.336198+0.00128913\ttest-merror:0.355568+0.00464521\n",
      "[100]\ttrain-merror:0.329422+0.00214505\ttest-merror:0.35299+0.00377903\n",
      "[150]\ttrain-merror:0.324117+0.00156501\ttest-merror:0.351469+0.00395345\n",
      "[200]\ttrain-merror:0.31863+0.00104249\ttest-merror:0.350676+0.00364439\n",
      "[250]\ttrain-merror:0.312729+0.00118844\ttest-merror:0.350874+0.00249209\n"
     ]
    }
   ],
   "source": [
    "cv_result = xgb.cv(xgb_params, \n",
    "                   dtrain_LM, \n",
    "                   num_boost_round=1200, # increase to have better results (~700)\n",
    "                   verbose_eval=50,\n",
    "                   early_stopping_rounds=50\n",
    "                  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
